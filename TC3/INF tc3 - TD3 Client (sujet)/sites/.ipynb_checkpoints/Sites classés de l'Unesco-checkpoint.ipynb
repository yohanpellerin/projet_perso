{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center style=\"color: #66d\">Projet B - Sites classés de l'Unesco</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Origine des données - DBPedia\n",
    "\n",
    "Cette année, le principe de chacun des projets consiste à récupérer des données sur DBPedia. \n",
    "\n",
    "<img src=\"DBPedia.png\" width=\"120\">\n",
    "\n",
    "DBpedia est un projet universitaire et communautaire d'exploration et extraction automatique de données dérivées de Wikipédia. Son principe est de proposer une version structurée et sous forme de données normalisées au format RDF des contenus encyclopédiques de chaque page de Wikipédia. Il existe plusieurs versions de DBpedia et dans plusieurs langues. Les trois versions principales sont la version anglaise (http://dbpedia.org/sparql), la versions française (http://fr.dbpedia.org) et la version allemande (http://de.dbpedia.org/).\n",
    "\n",
    "La version qui a été utilisée pour récupérer les données qui vous sont fournies est la version anglaise c’est à dire http://dbpedia.org/ car c’est la plus complète. Cependant il est tout à fait possible d’adapter les différentes requêtes aux autres chapitres multilingues de DBpedia (ex: la version française) en tenant compte des différences entre les chapitres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Format des données - RDF\n",
    "\n",
    "Sur DBPedia, les données sont représentées au format\n",
    "<a href=\"https://fr.wikipedia.org/wiki/Resource_Description_Framework\">RDF</a>\n",
    "(Resource Description Framework). RDF est un modèle de graphe destiné à décrire de façon formelle des ressources et leurs métadonnées, de façon à permettre le traitement automatique de telles descriptions. Développé par le <a href=\"https://www.w3.org/\">World Wide Web Consortium</a> (W3C en abrégé), RDF est un des langages de base du Web sémantique.\n",
    "\n",
    "<img src=\"Rdf_logo.svg\" width=\"100\">\n",
    "\n",
    "Un document RDF est constitué d'un ensemble de triplets. Un triplet RDF est une association (sujet, prédicat, objet).\n",
    "\n",
    "* Le sujet représente la ressource à décrire.\n",
    "* Le prédicat est une propriété de la ressource.\n",
    "* L’objet donne la valeur de la propriété, et peut correspondre à une donnée numérique ou textuelle, ou à autre ressource.\n",
    "\n",
    "Les ressources et les prédicats sont représentés à l'aide d'une URL.\n",
    "\n",
    "Exemple : pour observer l'ensemble des propriétés de la ressource <code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Carthage_amphitheatre&gt;</code> avec leur valeur il suffit d'actionner le lien : http://dbpedia.org/resource/Carthage_amphitheatre\n",
    "\n",
    "On y apprend ainsi que :<br>\n",
    "<code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Carthage_amphitheatre&gt;</code> <code>dbp:location dbr:Tunisia</code>\n",
    "\n",
    "Ce qui peut s'exprimer en français par : L'amphitéâtre de Carthage est situé (dbpedia property : location) en Tunisie (dbpedia resource : Tunisia).\n",
    "\n",
    "Note: pour représenter les URLs des ressources et des propriétés, DBPedia utilise un certain nombre de préfixes prédéfinis. Ainsi l'URL <code>&lt;dbr:Tunisia&gt;</code> correspond à <code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Tunisia&gt;</code> obtenue en remplaçant le préfixe par sa valeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Récupération de données sur DBPedia - SPARQL\n",
    "\n",
    "<a href=\"https://fr.wikipedia.org/wiki/SPARQL\">SPARQL</a> est un langage de requête et un protocole qui permet de rechercher, d'ajouter, de modifier ou de supprimer des données RDF disponibles à travers Internet. Son nom est un acronyme récursif qui signifie : \"SPARQL Protocol And RDF Query Language\".\n",
    "\n",
    "Voici un exemple simple de requête SPARQL, qui s'interprète comme \"Quelle est la ressource dont le sujet principal est l'amphitéâtre de Carthage ?\", et va nous retourner l'adresse de la page wikipédia consacrée à l'amphitéâtre de Carthage :\n",
    "\n",
    "<code>SELECT ?wiki  WHERE { </code><code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Carthage_amphitheatre&gt;</code><code>  foaf:isPrimaryTopicOf  ?wiki  }</code>\n",
    "\n",
    "Il est possible de soumettre de telles requêtes sur le point d'accès dédié de DBPedia : http://dbpedia.org/sparql.\n",
    "\n",
    "<div style=\"background-color:#eef;padding:10px;border-radius:3px; margin-top: 1.33em\">\n",
    "Soumettez la requête précédente via le point d'accès SPARQL de DBPedia pour observer\n",
    "la réponse obtenue, et vérifier que l'information retournée correspond bien à l'adresse de la page wikipédia demandée : <code>http://en.wikipedia.org/wiki/Carthage_amphitheatre</code>.\n",
    "</div>\n",
    "\n",
    "SPARQL est un langage puissant, qui permet d'émettre des requêtes complexes. Pour plus d'informations sur SPARQL et la façon d'utiliser DBPedia, il ne sera pas inutile de consulter le <a href=\"http://fr.dbpedia.org/sparqlTuto/tutoSparql.html\">tutoriel en ligne</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Données sur les sites\n",
    "\n",
    "Les ressources concernant les sites Unesco disponibles sur DBPedia sont du type <a href=\"https://dbpedia.org/ontology/WorldHeritageSite\"><code>dbo:WorldHeritageSite</code></a>. La requête suivante en demande la liste, avec leur nom, l'adresse de la page Wikipédia qui les décrit, leur latitude, longitude, le texte et l'URL de l'image qui les décrivent sur Wikipédia :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "sparql"
   },
   "source": [
    "SELECT DISTINCT ?site ?name ?wiki ?lat ?lon ?abstract ?photo\n",
    "WHERE {\n",
    "    ?site rdf:type dbo:WorldHeritageSite ;\n",
    "        rdfs:label ?name ;\n",
    "        foaf:isPrimaryTopicOf ?wiki ;\n",
    "        geo:lat ?lat ;\n",
    "        geo:long ?lon ;\n",
    "        dbo:abstract ?abstract ;\n",
    "        dbo:thumbnail ?photo\n",
    "FILTER langMatches(lang(?abstract), 'fr')\n",
    "FILTER langMatches(lang(?name), 'fr')\n",
    "}\n",
    "ORDER BY (?name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque importante__\n",
    "\n",
    "<p>Cela n'est pas nécessaire dans l'immédiat, mais si vous désirez réinitialiser le contenu de votre base de données, il faudra exécuter dans l'ordre, l'ensemble des cellules présentes dans la suite de ce notebook. Si les données sources ont été modifiées, il faudra peut-être intervenir à la marge sur certaines parties du code de nettoyage des données.</p>\n",
    "\n",
    "<p>De même, pour compléter et/ou modifier vos données, vous devrez modifier la requête SPARQL présente dans la cellule ci-dessus, et éventuellement nettoyer les nouvelles données obtenues en complétant le notebook avec le code python nécessaire.\n",
    "</p>\n",
    "\n",
    "<p>Toutefois, avant de vous aventurer à modifier la requête SPARQL, il sera pertinent de tester votre nouvelle requête via le point d'entrée interactif de DBPedia, en ajoutant une clause LIMIT(10) par exemple, pour éviter de surcharger le serveur, et d'être obligé d'attendre les résultats trop longtemps.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.1 Enregistrement du notebook et récupération de son nom dans la variable notebook_name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
       "IPython.notebook.save_notebook()\n",
       "\n",
       "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"notebook_name = \" + \"\\\"\"+thename+\"\\\"\";\n",
       "kernel.execute(command);\n",
       "element.text(command)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
    "IPython.notebook.save_notebook()\n",
    "\n",
    "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"notebook_name = \" + \"\\\"\"+thename+\"\\\"\";\n",
    "kernel.execute(command);\n",
    "element.text(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.2 Définition des fonctions utilisées par la suite__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Emission d'un requête SPARQL vers le point d'entrée DBPedia\n",
    "# et récupération du résultat dans un fichier csv\n",
    "#\n",
    "# id : metadata.id de la cellule avec la requête SPARQL, et nom du fichier csv\n",
    "#\n",
    "def dbpedia_sparql_to_csv(cell_id):\n",
    "\n",
    "    query = get_cell_by_id(cell_id)['source']\n",
    "    url = display_dbpedia_links(query)['csv']\n",
    "    http_request_to_file(url,'{}.csv'.format(cell_id))\n",
    "\n",
    "#\n",
    "# Récupère une cellule du présent notebook\n",
    "#\n",
    "def get_cell_by_id(cell_id):\n",
    "\n",
    "    # https://discourse.jupyter.org/t/extract-specific-cells-from-students-notebooks/7951/4\n",
    "    import os\n",
    "    import nbformat as nbf\n",
    "    filename = \"{}.ipynb\".format(notebook_name)\n",
    "    notebook = nbf.read(filename, nbf.NO_CONVERT)\n",
    "    return [c for c in notebook.cells if 'id' in c['metadata'] and c['metadata']['id'] == cell_id][0]\n",
    "\n",
    "#\n",
    "# Renvoie l'url d'une requête vers le point d'entré SPARQL de DBPedia\n",
    "#\n",
    "def dbpedia_sparql_url(query,fmt):\n",
    "\n",
    "    # https://stackoverflow.com/questions/40557606/how-to-url-encode-in-python-3\n",
    "    from urllib.parse import urlencode, quote_plus\n",
    "    url = \"https://dbpedia.org/sparql\"\n",
    "    params = {\n",
    "        \"default-graph-uri\" : \"http://dbpedia.org\",\n",
    "        \"query\" : query,\n",
    "        \"format\" : fmt,\n",
    "        \"timeout\" : 30000,\n",
    "        \"signal_void\" : \"on\",\n",
    "        \"signal_unconnected\" : \"on\"\n",
    "    }\n",
    "    return \"{}?{}\".format(url,urlencode(params,quote_via=quote_plus))\n",
    "\n",
    "#\n",
    "# Affiche et renvoie les liens pour une requête SPARQL sur le point d'entrée DBPedia\n",
    "# avec un résultat au format html, json, ou csv\n",
    "#\n",
    "def display_dbpedia_links(query):\n",
    "    \n",
    "    html_url = dbpedia_sparql_url(query,'text/html')\n",
    "    json_url = dbpedia_sparql_url(query,'application/sparql-results+json')\n",
    "    csv_url = dbpedia_sparql_url(query,'text/csv')\n",
    "    \n",
    "    # https://stackoverflow.com/questions/48248987/inject-execute-js-code-to-ipython-notebook-and-forbid-its-further-execution-on-p\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    html_link = '<a href=\"{}\">HTML</a>'.format(html_url)\n",
    "    json_link = '<a href=\"{}\">JSON</a>'.format(json_url)\n",
    "    csv_link = '<a href=\"{}\">CSV</a>'.format(csv_url)\n",
    "\n",
    "    display(HTML('Requêtes : {}&nbsp;&nbsp;{}&nbsp;&nbsp;{}'.format(html_link,json_link,csv_link)))\n",
    "\n",
    "    return { \"html\": html_url, \"json\": json_url, \"csv\": csv_url}\n",
    "\n",
    "#\n",
    "# Emet une requête http et enregistre le résultat dans un fichier\n",
    "#\n",
    "def http_request_to_file(url,filename):\n",
    "    \n",
    "    # https://stackoverflow.com/questions/645312/what-is-the-quickest-way-to-http-get-in-python\n",
    "    import urllib.request\n",
    "    contents = urllib.request.urlopen(url).read()\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "        f.write(contents)\n",
    "\n",
    "#\n",
    "# Vérifie si une chaîne peut être convertie en float\n",
    "#\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.3 Récupération des données brutes provenant de DBPedia__\n",
    "\n",
    "Cette cellule envoie la requête SPARQL au serveur DBPedia, et enregistre le résultat dans le fichier sparql.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Requêtes : <a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fsite+%3Fname+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fsite+rdf%3Atype+dbo%3AWorldHeritageSite+%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0AFILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=text%2Fhtml&timeout=30000&signal_void=on&signal_unconnected=on\">HTML</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fsite+%3Fname+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fsite+rdf%3Atype+dbo%3AWorldHeritageSite+%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0AFILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=application%2Fsparql-results%2Bjson&timeout=30000&signal_void=on&signal_unconnected=on\">JSON</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fsite+%3Fname+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fsite+rdf%3Atype+dbo%3AWorldHeritageSite+%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0AFILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=text%2Fcsv&timeout=30000&signal_void=on&signal_unconnected=on\">CSV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_filename = 'sparql'\n",
    "dbpedia_sparql_to_csv(raw_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.4 Nettoyage des données__\n",
    "\n",
    "La cellule suivante relit le fichier des données brutes dans le dictionnaire nommé <code>sites</code>, puis les cellules consécutives modifient ces données en mémoire pour les nettoyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Lecture du fichier d'origine, avec suppression des doublons\n",
    "#\n",
    "import csv\n",
    "\n",
    "sites = {}\n",
    "skip = [\n",
    "    'Aapravasi Ghat',\n",
    "    'Angkor Vat',\n",
    "    'Bahla',\n",
    "    'Bauhaus',\n",
    "    'Boğazkale',\n",
    "    'Béguinage',\n",
    "    'Cappadoce',\n",
    "    'Durmitor',\n",
    "    'Eridu',\n",
    "    'Goiás',\n",
    "    'Goiás (ville)',\n",
    "    'Lavaux (Léglise)',\n",
    "    'Morne Brabant',\n",
    "    'Forêt atlantique',\n",
    "    'Pantanal',\n",
    "    'Microrégion de la Chapada dos Veadeiros',\n",
    "    'Parc national de Durmitor',\n",
    "    'Pergame',\n",
    "    'Salzkammergut',\n",
    "    'Suse (Iran)',\n",
    "    'Villa Adriana (frazione)',\n",
    "    \"Fès el-Bali\",\n",
    "    'Grand-Place de Bruxelles',\n",
    "    'Mont Kōya',\n",
    "    'Nécropole de Gizeh',\n",
    "]\n",
    "replace = [\n",
    "    'Angkor',\n",
    "    'Bergama',\n",
    "    'Bâtiment du Bauhaus (Dessau)',\n",
    "    'Béguinages flamands',\n",
    "    'Cappadoce (thème)',\n",
    "    'Douro (DOC)',\n",
    "    'Fort de Bahla',\n",
    "    'Hattusa',\n",
    "    'Jeju',\n",
    "    'Lavaux',\n",
    "    'Mont Lu',\n",
    "    'Pantanais du Mato Grosso do Sul',\n",
    "    'Parc national de la Chapada dos Veadeiros',\n",
    "    'Rynek Starego Miasta (Varsovie)',\n",
    "    'Różan (ville)',\n",
    "    'Ur (Mésopotamie)',\n",
    "    'Île volcanique et tunnels de lave de Jeju',\n",
    "\n",
    "]\n",
    "delete = [\n",
    "    'Suse (Italie)',\n",
    "    'Ur (Pyrénées-Orientales)',\n",
    "    'Vieille ville de Varsovie',\n",
    "    'Vignoble de la vallée du Haut Douro',\n",
    "    \"Villa d'Hadrien\",\n",
    "    'Kōya',\n",
    "    'Médina de Fès',\n",
    "    'Parc de Bruxelles',\n",
    "    'Pyramide (architecture)',\n",
    "    \"Pyramides d'Égypte\",\n",
    "    \"Pyramides de Gizeh\"\n",
    "    \n",
    "]\n",
    "\n",
    "with open('{}.csv'.format(raw_filename),encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=',')\n",
    "    for row in reader:\n",
    "        if not row['name'] in sites:\n",
    "            sites[row['name']] = row\n",
    "        elif row['name'] == 'Aapravasi Ghat' and row['name'] in row['abstract']:\n",
    "            sites[row['name']] = row\n",
    "        elif row['name'] in skip:\n",
    "            pass\n",
    "        elif row['name'] in replace:\n",
    "            sites[row['name']] = row\n",
    "        elif row['name'] in delete:\n",
    "            del sites[row['name']]\n",
    "        else:\n",
    "            print(row['name'], '\\n',sites[row['name']]['abstract'], '\\n',row['abstract'],'\\n')\n",
    "            break\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/Angkor Angkor\n",
      "http://dbpedia.org/resource/Angkor Angkor Vat\n",
      "http://dbpedia.org/resource/Beguinage Béguinage\n",
      "http://dbpedia.org/resource/Beguinage Béguinages flamands\n",
      "http://dbpedia.org/resource/Cappadocia Cappadoce\n",
      "http://dbpedia.org/resource/Cappadocia Cappadoce (thème)\n",
      "http://dbpedia.org/resource/Bahla_Fort Bahla\n",
      "http://dbpedia.org/resource/Bahla_Fort Fort de Bahla\n",
      "http://dbpedia.org/resource/Goiás,_Goiás Goiás\n",
      "http://dbpedia.org/resource/Goiás,_Goiás Goiás (ville)\n",
      "http://dbpedia.org/resource/Hattusa Boğazkale\n",
      "http://dbpedia.org/resource/Hattusa Hattusa\n",
      "http://dbpedia.org/resource/Lavaux Lavaux\n",
      "http://dbpedia.org/resource/Lavaux Lavaux (Léglise)\n",
      "http://dbpedia.org/resource/Pantanal Pantanais du Mato Grosso do Sul\n",
      "http://dbpedia.org/resource/Pantanal Pantanal\n",
      "http://dbpedia.org/resource/Durmitor Durmitor\n",
      "http://dbpedia.org/resource/Durmitor Parc national de Durmitor\n",
      "http://dbpedia.org/resource/Chapada_dos_Veadeiros_National_Park Microrégion de la Chapada dos Veadeiros\n",
      "http://dbpedia.org/resource/Chapada_dos_Veadeiros_National_Park Parc national de la Chapada dos Veadeiros\n",
      "http://dbpedia.org/resource/Pergamon Bergama\n",
      "http://dbpedia.org/resource/Pergamon Pergame\n",
      "http://dbpedia.org/resource/Mount_Lu Mont Lu\n",
      "http://dbpedia.org/resource/Mount_Lu Różan (ville)\n",
      "http://dbpedia.org/resource/Bonin_Islands Archipel Ogasawara\n",
      "http://dbpedia.org/resource/Bonin_Islands Îles Bonin\n"
     ]
    }
   ],
   "source": [
    "# suppression des doublons sur l'id\n",
    "sites_by_id = {}\n",
    "for s in sites:\n",
    "    id = sites[s]['site']\n",
    "    if not id in sites_by_id:\n",
    "        sites_by_id[id] = sites[s]\n",
    "    else:\n",
    "        print(sites_by_id[id]['site'],sites_by_id[id]['name'])\n",
    "        print(sites[s]['site'],sites[s]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.5 Ecriture du fichier des données nettoyées__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site', 'name', 'wiki', 'lat', 'lon', 'abstract', 'photo']\n"
     ]
    }
   ],
   "source": [
    "#           \n",
    "# Ecriture du fichier csv à importer dans la base de données\n",
    "#\n",
    "fieldnames = list(sites['Aapravasi Ghat'].keys())\n",
    "\n",
    "print(fieldnames)\n",
    "\n",
    "with open('sites.csv', 'w', encoding='utf-8', newline='\\n') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    for v in sites_by_id:\n",
    "        writer.writerow({f: sites_by_id[v][f] for f in fieldnames})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.6 Création / mise à jour de la base de données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# relecture du fichier de données\n",
    "sites = {}\n",
    "\n",
    "with open('sites.csv',encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=';')\n",
    "    for row in reader:\n",
    "        sites[row['name']] = row\n",
    "\n",
    "fieldnames = list(sites['Aapravasi Ghat'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Mise à jour de la base de données\n",
    "#\n",
    "import sqlite3\n",
    "\n",
    "unesco_dbname = 'sites.db'\n",
    "conn = sqlite3.connect(unesco_dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"DROP TABLE IF EXISTS sites\")\n",
    "conn.commit()\n",
    "\n",
    "c.execute('''CREATE TABLE \"sites\" (\n",
    "    `site` TEXT PRIMARY KEY,\n",
    "    `name` TEXT,\n",
    "    `wiki` TEXT,\n",
    "    `lat` REAL,\n",
    "    `lon` REAL,\n",
    "    `abstract` TEXT,\n",
    "    `photo` TEXT\n",
    ")''')\n",
    "conn.commit()\n",
    "\n",
    "request = 'INSERT INTO sites ({}) VALUES ({})'.format(','.join(fieldnames),','.join(['?']*len(fieldnames)))\n",
    "for s in sites:\n",
    "    c.execute(request,[sites[s][k] for k in fieldnames])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
