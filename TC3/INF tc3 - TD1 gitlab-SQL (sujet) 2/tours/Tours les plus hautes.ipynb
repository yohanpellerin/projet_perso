{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center style=\"color: #66d\">Projet C - Tours les plus hautes</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Origine des données - DBPedia\n",
    "\n",
    "Cette année, le principe de chacun des projets consiste à récupérer des données sur DBPedia. \n",
    "\n",
    "<img src=\"DBPedia.png\" width=\"120\">\n",
    "\n",
    "DBpedia est un projet universitaire et communautaire d'exploration et extraction automatique de données dérivées de Wikipédia. Son principe est de proposer une version structurée et sous forme de données normalisées au format RDF des contenus encyclopédiques de chaque page de Wikipédia. Il existe plusieurs versions de DBpedia et dans plusieurs langues. Les trois versions principales sont la version anglaise (http://dbpedia.org/sparql), la versions française (http://fr.dbpedia.org) et la version allemande (http://de.dbpedia.org/).\n",
    "\n",
    "La version qui a été utilisée pour récupérer les données qui vous sont fournies est la version anglaise c’est à dire http://dbpedia.org/ car c’est la plus complète. Cependant il est tout à fait possible d’adapter les différentes requêtes aux autres chapitres multilingues de DBpedia (ex: la version française) en tenant compte des différences entre les chapitres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Format des données - RDF\n",
    "\n",
    "Sur DBPedia, les données sont représentées au format\n",
    "<a href=\"https://fr.wikipedia.org/wiki/Resource_Description_Framework\">RDF</a>\n",
    "(Resource Description Framework). RDF est un modèle de graphe destiné à décrire de façon formelle des ressources et leurs métadonnées, de façon à permettre le traitement automatique de telles descriptions. Développé par le <a href=\"https://www.w3.org/\">World Wide Web Consortium</a> (W3C en abrégé), RDF est un des langages de base du Web sémantique.\n",
    "\n",
    "<img src=\"Rdf_logo.svg\" width=\"100\">\n",
    "\n",
    "Un document RDF est constitué d'un ensemble de triplets. Un triplet RDF est une association (sujet, prédicat, objet).\n",
    "\n",
    "* Le sujet représente la ressource à décrire.\n",
    "* Le prédicat est une propriété de la ressource.\n",
    "* L’objet donne la valeur de la propriété, et peut correspondre à une donnée numérique ou textuelle, ou à autre ressource.\n",
    "\n",
    "Les ressources et les prédicats sont représentés à l'aide d'une URL.\n",
    "\n",
    "Exemple : pour observer l'ensemble des propriétés de la ressource <code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Burj_Khalifa&gt;</code> avec leur valeur il suffit d'actionner le lien : http://dbpedia.org/resource/Burj_Khalifa\n",
    "\n",
    "On y apprend ainsi que :<br>\n",
    "<code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Burj_Khalifa&gt;</code> <code>dbo:elevatorCount 57</code>\n",
    "\n",
    "Ce qui peut s'exprimer en français par : La tour Burj Khalifa possède 57 ascenseurs (dbpedia ontology : elevatorCount).\n",
    "\n",
    "Note: pour représenter les URLs des ressources et des propriétés, DBPedia utilise un certain nombre de préfixes prédéfinis. Ainsi l'URL <code>&lt;dbo:elevatorCount&gt;</code> correspond à <code>&lt;http://</code><code>dbpedia.org</code><code>/ontology/elevatorCount&gt;</code> obtenue en remplaçant le préfixe par sa valeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Récupération de données sur DBPedia - SPARQL\n",
    "\n",
    "<a href=\"https://fr.wikipedia.org/wiki/SPARQL\">SPARQL</a> est un langage de requête et un protocole qui permet de rechercher, d'ajouter, de modifier ou de supprimer des données RDF disponibles à travers Internet. Son nom est un acronyme récursif qui signifie : \"SPARQL Protocol And RDF Query Language\".\n",
    "\n",
    "Voici un exemple simple de requête SPARQL, qui s'interprète comme \"Quelle est la ressource dont le sujet principal est la tour Burj Khalifa ?\", et va nous retourner l'adresse de la page wikipédia consacrée à cette tour :\n",
    "\n",
    "<code>SELECT ?wiki  WHERE { </code><code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Burj_Khalifa&gt;</code><code>  foaf:isPrimaryTopicOf  ?wiki  }</code>\n",
    "\n",
    "Il est possible de soumettre de telles requêtes sur le point d'accès dédié de DBPedia : http://dbpedia.org/sparql.\n",
    "\n",
    "<div style=\"background-color:#eef;padding:10px;border-radius:3px; margin-top: 1.33em\">\n",
    "Soumettez la requête précédente via le point d'accès SPARQL de DBPedia pour observer\n",
    "la réponse obtenue, et vérifier que l'information retournée correspond bien à l'adresse de la page wikipédia demandée : <code>http://en.wikipedia.org/wiki/Burj_Khalifa</code>.\n",
    "</div>\n",
    "\n",
    "SPARQL est un langage puissant, qui permet d'émettre des requêtes complexes. Pour plus d'informations sur SPARQL et la façon d'utiliser DBPedia, il ne sera pas inutile de consulter le <a href=\"http://fr.dbpedia.org/sparqlTuto/tutoSparql.html\">tutoriel en ligne</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Données sur les tours\n",
    "\n",
    "Les ressources concernant les tours disponibles sur DBPedia sont du type <a href=\"http://dbpedia.org/class/yago/Skyscraper104233124\"><code>yago:Skyscraper104233124</code></a>. La requête suivante demande la liste de celles dont la hauteur est supérieure à 150m, avec leur nom, l'adresse de la page Wikipédia qui les décrit, leur latitude, longitude, hauteur, date d'achèvement, et le texte et l'URL de l'image qui les décrivent sur Wikipédia :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "sparql"
   },
   "source": [
    "SELECT DISTINCT ?building ?name ?wiki ?lat ?lon ?height ?date ?abstract ?photo\n",
    "WHERE {\n",
    "    ?building rdf:type yago:Skyscraper104233124;\n",
    "        rdfs:label ?name ;\n",
    "        foaf:isPrimaryTopicOf ?wiki ;\n",
    "        geo:lat ?lat ;\n",
    "        geo:long ?lon ;\n",
    "        dbo:height ?height;\n",
    "        dbp:completionDate ?date;\n",
    "        dbo:abstract ?abstract ;\n",
    "        dbo:thumbnail ?photo\n",
    "FILTER langMatches(lang(?abstract), 'fr')\n",
    "FILTER langMatches(lang(?name), 'fr')\n",
    "FILTER (?height > 150)\n",
    "}\n",
    "ORDER BY DESC(?height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque importante__\n",
    "\n",
    "<p>Cela n'est pas nécessaire dans l'immédiat, mais si vous désirez réinitialiser le contenu de votre base de données, il faudra exécuter dans l'ordre, l'ensemble des cellules présentes dans la suite de ce notebook. Si les données sources ont été modifiées, il faudra peut-être intervenir à la marge sur certaines parties du code de nettoyage des données.</p>\n",
    "\n",
    "<p>De même, pour compléter et/ou modifier vos données, vous devrez modifier la requête SPARQL présente dans la cellule ci-dessus, et éventuellement nettoyer les nouvelles données obtenues en complétant le notebook avec le code python nécessaire.\n",
    "</p>\n",
    "\n",
    "<p>Toutefois, avant de vous aventurer à modifier la requête SPARQL, il sera pertinent de tester votre nouvelle requête via le point d'entrée interactif de DBPedia, en ajoutant une clause LIMIT(10) par exemple, pour éviter de surcharger le serveur, et d'être obligé d'attendre les résultats trop longtemps.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.1 Enregistrement du notebook et récupération de son nom dans la variable notebook_name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
       "IPython.notebook.save_notebook()\n",
       "\n",
       "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"notebook_name = \" + \"\\\"\"+thename+\"\\\"\";\n",
       "kernel.execute(command);\n",
       "element.text(command)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
    "IPython.notebook.save_notebook()\n",
    "\n",
    "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"notebook_name = \" + \"\\\"\"+thename+\"\\\"\";\n",
    "kernel.execute(command);\n",
    "element.text(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.2 Définition des fonctions utilisées par la suite__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Emission d'un requête SPARQL vers le point d'entrée DBPedia\n",
    "# et récupération du résultat dans un fichier csv\n",
    "#\n",
    "# id : metadata.id de la cellule avec la requête SPARQL, et nom du fichier csv\n",
    "#\n",
    "def dbpedia_sparql_to_csv(cell_id):\n",
    "\n",
    "    query = get_cell_by_id(cell_id)['source']\n",
    "    url = display_dbpedia_links(query)['csv']\n",
    "    http_request_to_file(url,'{}.csv'.format(cell_id))\n",
    "\n",
    "#\n",
    "# Récupère une cellule du présent notebook\n",
    "#\n",
    "def get_cell_by_id(cell_id):\n",
    "\n",
    "    # https://discourse.jupyter.org/t/extract-specific-cells-from-students-notebooks/7951/4\n",
    "    import os\n",
    "    import nbformat as nbf\n",
    "    filename = \"{}.ipynb\".format(notebook_name)\n",
    "    notebook = nbf.read(filename, nbf.NO_CONVERT)\n",
    "    return [c for c in notebook.cells if 'id' in c['metadata'] and c['metadata']['id'] == cell_id][0]\n",
    "\n",
    "#\n",
    "# Renvoie l'url d'une requête vers le point d'entré SPARQL de DBPedia\n",
    "#\n",
    "def dbpedia_sparql_url(query,fmt):\n",
    "\n",
    "    # https://stackoverflow.com/questions/40557606/how-to-url-encode-in-python-3\n",
    "    from urllib.parse import urlencode, quote_plus\n",
    "    url = \"https://dbpedia.org/sparql\"\n",
    "    params = {\n",
    "        \"default-graph-uri\" : \"http://dbpedia.org\",\n",
    "        \"query\" : query,\n",
    "        \"format\" : fmt,\n",
    "        \"timeout\" : 30000,\n",
    "        \"signal_void\" : \"on\",\n",
    "        \"signal_unconnected\" : \"on\"\n",
    "    }\n",
    "    return \"{}?{}\".format(url,urlencode(params,quote_via=quote_plus))\n",
    "\n",
    "#\n",
    "# Affiche et renvoie les liens pour une requête SPARQL sur le point d'entrée DBPedia\n",
    "# avec un résultat au format html, json, ou csv\n",
    "#\n",
    "def display_dbpedia_links(query):\n",
    "    \n",
    "    html_url = dbpedia_sparql_url(query,'text/html')\n",
    "    json_url = dbpedia_sparql_url(query,'application/sparql-results+json')\n",
    "    csv_url = dbpedia_sparql_url(query,'text/csv')\n",
    "    \n",
    "    # https://stackoverflow.com/questions/48248987/inject-execute-js-code-to-ipython-notebook-and-forbid-its-further-execution-on-p\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    html_link = '<a href=\"{}\">HTML</a>'.format(html_url)\n",
    "    json_link = '<a href=\"{}\">JSON</a>'.format(json_url)\n",
    "    csv_link = '<a href=\"{}\">CSV</a>'.format(csv_url)\n",
    "\n",
    "    display(HTML('Requêtes : {}&nbsp;&nbsp;{}&nbsp;&nbsp;{}'.format(html_link,json_link,csv_link)))\n",
    "\n",
    "    return { \"html\": html_url, \"json\": json_url, \"csv\": csv_url}\n",
    "\n",
    "#\n",
    "# Emet une requête http et enregistre le résultat dans un fichier\n",
    "#\n",
    "def http_request_to_file(url,filename):\n",
    "    \n",
    "    # https://stackoverflow.com/questions/645312/what-is-the-quickest-way-to-http-get-in-python\n",
    "    import urllib.request\n",
    "    contents = urllib.request.urlopen(url).read()\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "        f.write(contents)\n",
    "\n",
    "#\n",
    "# Vérifie si une chaîne peut être convertie en float\n",
    "#\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.3 Récupération des données brutes provenant de DBPedia__\n",
    "\n",
    "Cette cellule envoie la requête SPARQL au serveur DBPedia, et enregistre le résultat dans le fichier sparql.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Requêtes : <a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fbuilding+%3Fname+%3Fwiki+%3Flat+%3Flon+%3Fheight+%3Fdate+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fbuilding+rdf%3Atype+yago%3ASkyscraper104233124%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aheight+%3Fheight%3B%0A++++++++dbp%3AcompletionDate+%3Fdate%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0AFILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0AFILTER+%28%3Fheight+%3E+150%29%0A%7D%0AORDER+BY+DESC%28%3Fheight%29&format=text%2Fhtml&timeout=30000&signal_void=on&signal_unconnected=on\">HTML</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fbuilding+%3Fname+%3Fwiki+%3Flat+%3Flon+%3Fheight+%3Fdate+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fbuilding+rdf%3Atype+yago%3ASkyscraper104233124%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aheight+%3Fheight%3B%0A++++++++dbp%3AcompletionDate+%3Fdate%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0AFILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0AFILTER+%28%3Fheight+%3E+150%29%0A%7D%0AORDER+BY+DESC%28%3Fheight%29&format=application%2Fsparql-results%2Bjson&timeout=30000&signal_void=on&signal_unconnected=on\">JSON</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fbuilding+%3Fname+%3Fwiki+%3Flat+%3Flon+%3Fheight+%3Fdate+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fbuilding+rdf%3Atype+yago%3ASkyscraper104233124%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aheight+%3Fheight%3B%0A++++++++dbp%3AcompletionDate+%3Fdate%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0AFILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0AFILTER+%28%3Fheight+%3E+150%29%0A%7D%0AORDER+BY+DESC%28%3Fheight%29&format=text%2Fcsv&timeout=30000&signal_void=on&signal_unconnected=on\">CSV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_filename = 'sparql'\n",
    "dbpedia_sparql_to_csv(raw_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.4 Nettoyage des données__\n",
    "\n",
    "La cellule suivante relit le fichier des données brutes dans le dictionnaire nommé <code>sites</code>, puis les cellules consécutives modifient ces données en mémoire pour les nettoyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Lecture du fichier d'origine, avec suppression des doublons\n",
    "#\n",
    "import csv\n",
    "\n",
    "towers = {}\n",
    "with open('{}.csv'.format(raw_filename),encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=',')\n",
    "    for row in reader:\n",
    "        if not row['name'] in towers:\n",
    "            towers[row['name']] = row\n",
    "\n",
    "print(len(towers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Traitement des dates\n",
    "#\n",
    "month_names = {\n",
    "    'january': 'janvier',\n",
    "    'feb': 'février',\n",
    "    'february': 'février',\n",
    "    'march': 'mars',\n",
    "    'april': 'avril',\n",
    "    'may': 'mai',\n",
    "    'june': 'juin',\n",
    "    'july': 'juillet',\n",
    "    'august': 'août',\n",
    "    'september': 'septembre',\n",
    "    'october': 'octobre',\n",
    "    'november': 'novembre',\n",
    "    'december': 'décembre'\n",
    "}\n",
    "months = [\n",
    "    'janvier',\n",
    "    'février',\n",
    "    'mars',\n",
    "    'avril',\n",
    "    'mai',\n",
    "    'juin',\n",
    "    'juillet',\n",
    "    'août',\n",
    "    'septembre',\n",
    "    'octobre',\n",
    "    'novembre',\n",
    "    'décembre'\n",
    "]\n",
    "junk = [\n",
    "    \"Between December 1982 and \",\n",
    "    'early ',\n",
    "    'Summer ',\n",
    "    'The Residence ',\n",
    "    'Tower 1 - '\n",
    "]\n",
    "\n",
    "for t in [t for t in towers]:\n",
    "    date = towers[t]['date']\n",
    "    \n",
    "    jk = [j for j in junk if date.startswith(j)]\n",
    "    if ':' in date:\n",
    "        date = date.split(': ')[1]\n",
    "    elif len(jk):\n",
    "        date = date[len(jk[0]):]\n",
    "\n",
    "    chunks = date.split('-')\n",
    "    words = date.strip().split(' ')\n",
    "\n",
    "    if date == 'Abandoned' or date == \"N/A\":\n",
    "        del towers[t]\n",
    "        \n",
    "    # --05-27\n",
    "    elif towers[t]['name'] == 'Chrysler Building':\n",
    "        towers[t]['date'] = '27 mai 1930'\n",
    "        towers[t]['year'] = 1930\n",
    "        \n",
    "    # --04-11\n",
    "    elif towers[t]['name'] == 'Eighth Avenue Place':\n",
    "        towers[t]['date'] = '2014'\n",
    "        towers[t]['year'] = 2014\n",
    "        \n",
    "    # Topped out\n",
    "    elif towers[t]['name'] == 'Tianjin Tower':\n",
    "        towers[t]['date'] = '14 janvier 2010'\n",
    "        towers[t]['year'] = 2010\n",
    "\n",
    "    # Renovated in 2013\n",
    "    elif towers[t]['name'] == 'Commerce Square':\n",
    "        towers[t]['date'] = '1987'\n",
    "        towers[t]['year'] = 1987\n",
    "\n",
    "    # 20032005\n",
    "    elif towers[t]['name'] == 'Torres El Faro':\n",
    "        towers[t]['date'] = '2003'\n",
    "        towers[t]['year'] = 2003\n",
    "        \n",
    "    # 2009-10-01\n",
    "    elif len(chunks) == 3:\n",
    "        (y,m,d) = date.split('-')\n",
    "        towers[t]['date'] = \"{} {} {}\".format(d,months[int(m.lower())-1],y)\n",
    "        towers[t]['year'] = int(y)\n",
    "    \n",
    "    # January 2010\n",
    "    elif len(words) == 2:      \n",
    "        (m,y) = date.split(' ')\n",
    "        if m.lower() in month_names:\n",
    "            towers[t]['date'] = \"{} {}\".format(month_names[m.lower()],y)\n",
    "        else :\n",
    "            towers[t]['date'] = date\n",
    "        towers[t]['year'] = int(towers[t]['date'].split(' ')[1])\n",
    "\n",
    "    # 07 décembre 2017\n",
    "    elif len(words) == 3:\n",
    "        towers[t]['date'] = date\n",
    "        towers[t]['year'] = int(towers[t]['date'].split(' ')[2])\n",
    "    \n",
    "    # 1992\n",
    "    else:\n",
    "        towers[t]['date'] = date\n",
    "        towers[t]['year'] = int(date)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Certains enregistrement font référence à des photos non disponibles (404 Not Found)\n",
    "#\n",
    "# Cette cellule met manuellement à jour ces enregistrements avec des photos accessibles\n",
    "#\n",
    "photos = {\n",
    "\t\"Marina 101\": \"https://upload.wikimedia.org/wikipedia/en/thumb/6/60/Marina_101.jpg/360px-Marina_101.jpg\",\n",
    "\t\"Al Hamra Tower\": \"https://upload.wikimedia.org/wikipedia/en/thumb/4/42/Al_Hamra_Tower.jpg/360px-Al_Hamra_Tower.jpg\",\n",
    "\t\"23 Marina\": \"https://upload.wikimedia.org/wikipedia/en/thumb/5/59/23_Marina.jpg/360px-23_Marina.jpg\",\n",
    "\t\"ADNOC Headquarters\": \"https://s3.amazonaws.com/images.skyscrapercenter.com/thumbs/77043_500x650.jpg\",\n",
    "\t\"Al Yaqoub Tower\": \"https://upload.wikimedia.org/wikipedia/en/thumb/d/d5/Al_Yaqoub_Tower.jpg/360px-Al_Yaqoub_Tower.jpg\",\n",
    "\t\"The Index\": \"https://upload.wikimedia.org/wikipedia/en/thumb/f/fb/The_Index_Dubai.jpg/360px-The_Index_Dubai.jpg\",\n",
    "\t\"53W53\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/53w53Aug9.jpg/360px-53w53Aug9.jpg\",\n",
    "\t\"Borj-e Milad\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Tehran_Milad_Tower%2C_2019.jpg/360px-Tehran_Milad_Tower%2C_2019.jpg\",\n",
    "\t\"Almas Tower\": \"https://upload.wikimedia.org/wikipedia/en/thumb/4/4f/Almas_Tower.jpg/360px-Almas_Tower.jpg\",\n",
    "\t\"Kingdom Centre\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/%EC%82%AC%EC%9A%B0%EB%94%94_%EB%A6%AC%EC%95%BC%EB%93%9C%EC%9D%98_%ED%82%B9%EB%8D%A4%EC%84%BC%ED%84%B0_%28Riyard_Kondom_Tower%29_-_panoramio.jpg/360px-%EC%82%AC%EC%9A%B0%EB%94%94_%EB%A6%AC%EC%95%BC%EB%93%9C%EC%9D%98_%ED%82%B9%EB%8D%A4%EC%84%BC%ED%84%B0_%28Riyard_Kondom_Tower%29_-_panoramio.jpg\",\n",
    "\t\"Arraya 2\": \"https://archello.com/thumbs/images/2018/01/30/Overall-frm-Al-Hamra-Tower-DuskNM.1517302476.2633.jpg?fit=crop&w=300&h=518\",\n",
    "\t\"Aspire Tower\": \"https://s3.amazonaws.com/images.skyscrapercenter.com/thumbs/42715_500x650.jpg\",\n",
    "\t\"Bahria Town ICON\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Bahria_Icon_Karachi_3.jpg/360px-Bahria_Icon_Karachi_3.jpg\",\n",
    "\t\"Khalid Al Attar Tower 2\": \"https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Millennium_Hotel_Dubai.jpg/375px-Millennium_Hotel_Dubai.jpg\",\n",
    "\t\"Brisbane Skytower\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Skylines_of_Brisbane_from_Kangaroo_Point_Cliffs_Park%2C_2020%2C_03.jpg/360px-Skylines_of_Brisbane_from_Kangaroo_Point_Cliffs_Park%2C_2020%2C_03.jpg\",\n",
    "\t\"Tour Al Faisaliah\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/El_Faysaliah.jpg/375px-El_Faysaliah.jpg\",\n",
    "\t\"Bahrain Financial Harbour\": \"https://upload.wikimedia.org/wikipedia/en/thumb/5/53/Bahrain_Financial_Harbour_2020.jpg/360px-Bahrain_Financial_Harbour_2020.jpg\",\n",
    "\t\"Dubai Marriott Harbour Hotel & Suites\": \"https://cache.marriott.com/marriottassets/marriott/DXBHR/dxbhr-exterior-0050-ver-clsc.jpg?downsize=1440px:*\",\n",
    "\t\"Gramercy Residences\": \"https://upload.wikimedia.org/wikipedia/en/thumb/2/21/Gramercy_Residences_%28Century_City%2C_Makati_Ave._Cor._Kalayaan_Ave.%2C_Poblacion%2C_Makati%3B_2015-06-07%29.jpg/360px-Gramercy_Residences_%28Century_City%2C_Makati_Ave._Cor._Kalayaan_Ave.%2C_Poblacion%2C_Makati%3B_2015-06-07%29.jpg\",\n",
    "\t\"Menara Komtar Complex\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/2018_New_Year_Fireworks_in_George_Town%2C_Penang.jpg/375px-2018_New_Year_Fireworks_in_George_Town%2C_Penang.jpg\",\n",
    "\t\"Chelsea Tower\": \"https://upload.wikimedia.org/wikipedia/en/thumb/1/14/Chelsea_Tower.jpg/360px-Chelsea_Tower.jpg\",\n",
    "\t\"1717 Broadway\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/1717_Broadway_Feb_2017.png/360px-1717_Broadway_Feb_2017.png\",\n",
    "\t\"St-Francis Square\": \"https://upload.wikimedia.org/wikipedia/en/b/ba/BSA_Twin_Towers2012.jpg\",\n",
    "\t\"155 North Wacker\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/155NWacker.jpg/300px-155NWacker.jpg\",\n",
    "    \"The Clare\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/20080116_The_Clare_%40_The_Water_Tower.jpg/398px-20080116_The_Clare_%40_The_Water_Tower.jpg\",\n",
    "    \"Tour D2\": \"https://upload.wikimedia.org/wikipedia/fr/thumb/e/e4/Tour_D2_Juillet_2020.jpeg/390px-Tour_D2_Juillet_2020.jpeg\",\n",
    "    \"Tour Carpe Diem\": \"https://upload.wikimedia.org/wikipedia/fr/thumb/8/8d/Tour_Carpe_Diem_Janvier_2020.jpeg/390px-Tour_Carpe_Diem_Janvier_2020.jpeg\",\n",
    "\t\"Tour internationale de Téhéran\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/Tehran_Tower_-_panoramio.jpg/360px-Tehran_Tower_-_panoramio.jpg\",\n",
    "\t\"Palazzo Lombardia\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Piazza_Gae_Aulenti_with_Palazzo_Lombardia_cropped.jpg/360px-Piazza_Gae_Aulenti_with_Palazzo_Lombardia_cropped.jpg\",\n",
    "    \"Zifeng Tower\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Zifeng_Tower_2017.jpg/311px-Zifeng_Tower_2017.jpg\",\n",
    "}\n",
    "\n",
    "for t in photos:\n",
    "    if t in towers:\n",
    "        towers[t]['photo'] = photos[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Fonctions pour la recherche des images aux liens erronés\n",
    "#\n",
    "import urllib.parse\n",
    "import http.client\n",
    "import time\n",
    "\n",
    "#\n",
    "# envoi d'une requête hhtp\n",
    "#\n",
    "def http_request(url):\n",
    "    # print('hello from http_request',url)\n",
    "    \n",
    "    (baseurl,querystring) = url.split('?',1) if '?' in url else (url,'')\n",
    "    (protocol,netpath) = baseurl.split(':',1)\n",
    "    (_,__,server,path) = netpath.split('/',3)\n",
    "    path = '/'.join([urllib.parse.quote(chunk) for chunk in path.split('/')])\n",
    "\n",
    "    conn = http.client.HTTPSConnection(server)\n",
    "    conn.request('HEAD','/'+path+'?'+querystring)\n",
    "    r = conn.getresponse()\n",
    "    \n",
    "    if ( r.status == 200 ):\n",
    "        return 200\n",
    "    elif ( r.status == 404 ):\n",
    "        return 404\n",
    "    elif ( r.status == 302 ):\n",
    "        # print ('302', 'redirecting to ',r.headers['Location'])\n",
    "        return http_request(r.headers['Location'])\n",
    "    elif ( r.status == 301 ):\n",
    "        # print ('301', 'redirecting to ',r.headers['Location'])\n",
    "        return http_request(r.headers['Location'])\n",
    "    else:\n",
    "        return r.status\n",
    "\n",
    "#\n",
    "# liste des photos aux liens erronés\n",
    "#\n",
    "failed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Mise à jour de la liste des photos aux liens erronnés\n",
    "#\n",
    "# Pour parcourir l'ensemble des tours, modifier les variables start et end.\n",
    "#\n",
    "# ATTENTION : cette procédure est potentiellement très lente puisqu'elle effectue une requête\n",
    "# pour vérifier chacune des images, et il y a plusieurs centaines de tours...\n",
    "#\n",
    "start = 0\n",
    "end = 10\n",
    "\n",
    "for t in [t for t in towers][start:end]:\n",
    "    status = http_request(towers[t]['photo'])\n",
    "    if ( status == 404 ):\n",
    "        print (status,towers[t]['name'],'\\n',towers[t]['wiki'],'\\n')\n",
    "        if not t in failed :\n",
    "            failed.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/The_New_York_Times_Building New York Times Building\n",
      "http://dbpedia.org/resource/The_New_York_Times_Building One Times Square\n",
      "http://dbpedia.org/resource/Aqua_(skyscraper) Aqua (groupe)\n",
      "http://dbpedia.org/resource/Aqua_(skyscraper) Aqua (Chicago)\n",
      "http://dbpedia.org/resource/Hearst_Tower_(Manhattan) Hearst Tower (Charlotte)\n",
      "http://dbpedia.org/resource/Hearst_Tower_(Manhattan) Hearst Tower (New York)\n"
     ]
    }
   ],
   "source": [
    "# suppression des doublons sur l'id\n",
    "towers_by_id = {}\n",
    "for t in towers:\n",
    "    id = towers[t]['building']\n",
    "    if not id in towers_by_id:\n",
    "        towers_by_id[id] = towers[t]\n",
    "    else:\n",
    "        print(towers_by_id[id]['building'],towers_by_id[id]['name'])\n",
    "        print(towers[t]['building'],towers[t]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.5 Ecriture du fichier des données nettoyées__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['building', 'name', 'wiki', 'lat', 'lon', 'height', 'date', 'abstract', 'photo', 'year']\n"
     ]
    }
   ],
   "source": [
    "#           \n",
    "# Ecriture du fichier csv à importer dans la base de données\n",
    "#\n",
    "fieldnames = list(towers['Burj Khalifa'].keys())\n",
    "\n",
    "print(fieldnames)\n",
    "\n",
    "with open('tours.csv', 'w', encoding='utf-8', newline='\\n') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    for id in towers_by_id:\n",
    "        writer.writerow({f: towers_by_id[id][f] for f in fieldnames})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.6 Création / mise à jour de la base de données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# relecture du fichier de données\n",
    "towers = {}\n",
    "\n",
    "with open('tours.csv',encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=';')\n",
    "    for row in reader:\n",
    "        towers[row['name']] = row\n",
    "\n",
    "fieldnames = list(towers['Burj Khalifa'].keys())\n",
    "\n",
    "#\n",
    "# Mise à jour de la base de données\n",
    "#\n",
    "import sqlite3\n",
    "\n",
    "towers_dbname = 'tours.db'\n",
    "conn = sqlite3.connect(towers_dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"DROP TABLE IF EXISTS tours\")\n",
    "conn.commit()\n",
    "\n",
    "c.execute('''CREATE TABLE \"tours\" (\n",
    "    `building` TEXT PRIMARY KEY,\n",
    "    `name` TEXT,\n",
    "    `wiki` TEXT,\n",
    "    `lat` REAL,\n",
    "    `lon` REAL,\n",
    "    `height` REAL,\n",
    "    `date` TEXT,\n",
    "    `year` INTEGER,\n",
    "    `abstract` TEXT,\n",
    "    `photo` TEXT\n",
    ")''')\n",
    "conn.commit()\n",
    "\n",
    "request = 'INSERT INTO tours ({}) VALUES ({})'.format(','.join(fieldnames),','.join(['?']*len(fieldnames)))\n",
    "for t in towers:\n",
    "    c.execute(request,[towers[t][k] for k in fieldnames])\n",
    "    \n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
