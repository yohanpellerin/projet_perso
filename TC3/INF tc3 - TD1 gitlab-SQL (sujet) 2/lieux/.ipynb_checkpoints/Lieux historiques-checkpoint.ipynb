{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center style=\"color: #66d\">Projet E - Lieux historiques</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Origine des données - DBPedia\n",
    "\n",
    "Cette année, le principe de chacun des projets consiste à récupérer des données sur DBPedia. \n",
    "\n",
    "<img src=\"DBPedia.png\" width=\"120\">\n",
    "\n",
    "DBpedia est un projet universitaire et communautaire d'exploration et extraction automatique de données dérivées de Wikipédia. Son principe est de proposer une version structurée et sous forme de données normalisées au format RDF des contenus encyclopédiques de chaque page de Wikipédia. Il existe plusieurs versions de DBpedia et dans plusieurs langues. Les trois versions principales sont la version anglaise (http://dbpedia.org/sparql), la versions française (http://fr.dbpedia.org) et la version allemande (http://de.dbpedia.org/).\n",
    "\n",
    "La version qui a été utilisée pour récupérer les données qui vous sont fournies est la version anglaise c’est à dire http://dbpedia.org/ car c’est la plus complète. Cependant il est tout à fait possible d’adapter les différentes requêtes aux autres chapitres multilingues de DBpedia (ex: la version française) en tenant compte des différences entre les chapitres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Format des données - RDF\n",
    "\n",
    "Sur DBPedia, les données sont représentées au format\n",
    "<a href=\"https://fr.wikipedia.org/wiki/Resource_Description_Framework\">RDF</a>\n",
    "(Resource Description Framework). RDF est un modèle de graphe destiné à décrire de façon formelle des ressources et leurs métadonnées, de façon à permettre le traitement automatique de telles descriptions. Développé par le <a href=\"https://www.w3.org/\">World Wide Web Consortium</a> (W3C en abrégé), RDF est un des langages de base du Web sémantique.\n",
    "\n",
    "<img src=\"Rdf_logo.svg\" width=\"100\">\n",
    "\n",
    "Un document RDF est constitué d'un ensemble de triplets. Un triplet RDF est une association (sujet, prédicat, objet).\n",
    "\n",
    "* Le sujet représente la ressource à décrire.\n",
    "* Le prédicat est une propriété de la ressource.\n",
    "* L’objet donne la valeur de la propriété, et peut correspondre à une donnée numérique ou textuelle, ou à autre ressource.\n",
    "\n",
    "Les ressources et les prédicats sont représentés à l'aide d'une URL.\n",
    "\n",
    "Exemple : pour observer l'ensemble des propriétés de la ressource <code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Ambrussum&gt;</code> avec leur valeur il suffit d'actionner le lien : http://dbpedia.org/resource/Ambrussum\n",
    "\n",
    "Où on apprend par exemple :<br>\n",
    "<code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Ambrussum&gt;</code> <code>dbp:built 4</code>\n",
    "\n",
    "Ce qui peut s'exprimer en français par : \"Le site d'Ambrussum date (dbpedia property : built) d'il y a 4 siècles, soit du 2ème siècle avant J.C.\". Et oui, il y a ambiguïté sur la façon d'interpréter cette propriété, d'où la nécessité de nettoyer les données (cf. suite du notebook).\n",
    "\n",
    "Note: pour représenter les URLs des ressources et des propriétés, DBPedia utilise un certain nombre de préfixes prédéfinis. Ainsi l'URL <code>&lt;dbp:built&gt;</code> correspond à <code>&lt;http://</code><code>dbpedia.org</code><code>/property/built&gt;</code> obtenue en remplaçant le préfixe par sa valeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Récupération de données sur DBPedia - SPARQL\n",
    "\n",
    "<a href=\"https://fr.wikipedia.org/wiki/SPARQL\">SPARQL</a> est un langage de requête et un protocole qui permet de rechercher, d'ajouter, de modifier ou de supprimer des données RDF disponibles à travers Internet. Son nom est un acronyme récursif qui signifie : \"SPARQL Protocol And RDF Query Language\".\n",
    "\n",
    "Voici un exemple simple de requête SPARQL, qui s'interprète comme \"Quelle est la ressource dont le sujet principal est le site d'Ambrussum ?\", et va nous retourner l'adresse de la page wikipédia consacrée à ce site :\n",
    "\n",
    "<code>SELECT ?wiki  WHERE { </code><code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Ambrussum&gt;</code><code>  foaf:isPrimaryTopicOf  ?wiki  }</code>\n",
    "\n",
    "Il est possible de soumettre de telles requêtes sur le point d'accès dédié de DBPedia : http://dbpedia.org/sparql.\n",
    "\n",
    "<div style=\"background-color:#eef;padding:10px;border-radius:3px; margin-top: 1.33em\">\n",
    "Soumettez la requête précédente via le point d'accès SPARQL de DBPedia pour observer\n",
    "la réponse obtenue, et vérifier que l'information retournée correspond bien à l'adresse de la page wikipédia demandée : <code>http://en.wikipedia.org/wiki/Ambrussum</code>.\n",
    "</div>\n",
    "\n",
    "SPARQL est un langage puissant, qui permet d'émettre des requêtes complexes. Pour plus d'informations sur SPARQL et la façon d'utiliser DBPedia, il ne sera pas inutile de consulter le <a href=\"http://fr.dbpedia.org/sparqlTuto/tutoSparql.html\">tutoriel en ligne</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Données sur les lieux historiques\n",
    "\n",
    "Les ressources concernant les lieux historiques disponibles sur DBPedia sont du type <a href=\"http://dbpedia.org/ontology/Bridge\"><code>dbo:HistoricPlace</code></a>. La requête suivante demande la liste des lieux historiques, avec leur nom, leur situation, date de construction, l'adresse de la page Wikipédia qui les décrit, leur latitude, longitude, et le texte et l'URL de l'image qui les décrivent sur Wikipédia :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "sparql"
   },
   "source": [
    "SELECT DISTINCT ?place ?name ?location ?locname ?date ?wiki ?lat ?lon ?abstract ?photo\n",
    "WHERE {\n",
    "    ?place rdf:type dbo:HistoricPlace ;\n",
    "                   foaf:name ?name ;\n",
    "                   dbp:built ?date ;\n",
    "                   dbo:location ?location ;\n",
    "                   foaf:isPrimaryTopicOf ?wiki ;\n",
    "                   geo:lat ?lat ;\n",
    "                   geo:long ?lon ;\n",
    "                   dbo:thumbnail ?photo ;\n",
    "                   dbo:abstract ?abstract .\n",
    "    ?location rdfs:label ?locname\n",
    "  FILTER langMatches(lang(?abstract), 'fr')\n",
    "  FILTER langMatches(lang(?locname), 'fr')\n",
    "}\n",
    "ORDER BY (?name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque importante__\n",
    "\n",
    "<p>Cela n'est pas nécessaire dans l'immédiat, mais si vous désirez réinitialiser le contenu de votre base de données, il faudra exécuter dans l'ordre, l'ensemble des cellules présentes dans la suite de ce notebook. Si les données sources ont été modifiées, il faudra peut-être intervenir à la marge sur certaines parties du code de nettoyage des données.</p>\n",
    "\n",
    "<p>De même, pour compléter et/ou modifier vos données, vous devrez modifier la requête SPARQL présente dans la cellule ci-dessus, et éventuellement nettoyer les nouvelles données obtenues en complétant le notebook avec le code python nécessaire.\n",
    "</p>\n",
    "\n",
    "<p>Toutefois, avant de vous aventurer à modifier la requête SPARQL, il sera pertinent de tester votre nouvelle requête via le point d'entrée interactif de DBPedia, en ajoutant une clause LIMIT(10) par exemple, pour éviter de surcharger le serveur, et d'être obligé d'attendre les résultats trop longtemps.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.1 Enregistrement du notebook et récupération de son nom dans la variable notebook_name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
       "IPython.notebook.save_notebook()\n",
       "\n",
       "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"notebook_name = \" + \"\\\"\"+thename+\"\\\"\";\n",
       "kernel.execute(command);\n",
       "element.text(command)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
    "IPython.notebook.save_notebook()\n",
    "\n",
    "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"notebook_name = \" + \"\\\"\"+thename+\"\\\"\";\n",
    "kernel.execute(command);\n",
    "element.text(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.2 Définition des fonctions utilisées par la suite__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Emission d'un requête SPARQL vers le point d'entrée DBPedia\n",
    "# et récupération du résultat dans un fichier csv\n",
    "#\n",
    "# id : metadata.id de la cellule avec la requête SPARQL, et nom du fichier csv\n",
    "#\n",
    "def dbpedia_sparql_to_csv(cell_id):\n",
    "\n",
    "    query = get_cell_by_id(cell_id)['source']\n",
    "    url = display_dbpedia_links(query)['csv']\n",
    "    http_request_to_file(url,'{}.csv'.format(cell_id))\n",
    "\n",
    "#\n",
    "# Récupère une cellule du présent notebook\n",
    "#\n",
    "def get_cell_by_id(cell_id):\n",
    "\n",
    "    # https://discourse.jupyter.org/t/extract-specific-cells-from-students-notebooks/7951/4\n",
    "    import os\n",
    "    import nbformat as nbf\n",
    "    filename = \"{}.ipynb\".format(notebook_name)\n",
    "    notebook = nbf.read(filename, nbf.NO_CONVERT)\n",
    "    return [c for c in notebook.cells if 'id' in c['metadata'] and c['metadata']['id'] == cell_id][0]\n",
    "\n",
    "#\n",
    "# Renvoie l'url d'une requête vers le point d'entré SPARQL de DBPedia\n",
    "#\n",
    "def dbpedia_sparql_url(query,fmt):\n",
    "\n",
    "    # https://stackoverflow.com/questions/40557606/how-to-url-encode-in-python-3\n",
    "    from urllib.parse import urlencode, quote_plus\n",
    "    url = \"https://dbpedia.org/sparql\"\n",
    "    params = {\n",
    "        \"default-graph-uri\" : \"http://dbpedia.org\",\n",
    "        \"query\" : query,\n",
    "        \"format\" : fmt,\n",
    "        \"timeout\" : 30000,\n",
    "        \"signal_void\" : \"on\",\n",
    "        \"signal_unconnected\" : \"on\"\n",
    "    }\n",
    "    return \"{}?{}\".format(url,urlencode(params,quote_via=quote_plus))\n",
    "\n",
    "#\n",
    "# Affiche et renvoie les liens pour une requête SPARQL sur le point d'entrée DBPedia\n",
    "# avec un résultat au format html, json, ou csv\n",
    "#\n",
    "def display_dbpedia_links(query):\n",
    "    \n",
    "    html_url = dbpedia_sparql_url(query,'text/html')\n",
    "    json_url = dbpedia_sparql_url(query,'application/sparql-results+json')\n",
    "    csv_url = dbpedia_sparql_url(query,'text/csv')\n",
    "    \n",
    "    # https://stackoverflow.com/questions/48248987/inject-execute-js-code-to-ipython-notebook-and-forbid-its-further-execution-on-p\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    html_link = '<a href=\"{}\">HTML</a>'.format(html_url)\n",
    "    json_link = '<a href=\"{}\">JSON</a>'.format(json_url)\n",
    "    csv_link = '<a href=\"{}\">CSV</a>'.format(csv_url)\n",
    "\n",
    "    display(HTML('Requêtes : {}&nbsp;&nbsp;{}&nbsp;&nbsp;{}'.format(html_link,json_link,csv_link)))\n",
    "\n",
    "    return { \"html\": html_url, \"json\": json_url, \"csv\": csv_url}\n",
    "\n",
    "#\n",
    "# Emet une requête http et enregistre le résultat dans un fichier\n",
    "#\n",
    "def http_request_to_file(url,filename):\n",
    "    \n",
    "    # https://stackoverflow.com/questions/645312/what-is-the-quickest-way-to-http-get-in-python\n",
    "    import urllib.request\n",
    "    contents = urllib.request.urlopen(url).read()\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "        f.write(contents)\n",
    "\n",
    "#\n",
    "# Vérifie si une chaîne peut être convertie en float\n",
    "#\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.3 Récupération des données brutes provenant de DBPedia__\n",
    "\n",
    "Cette cellule envoie la requête SPARQL au serveur DBPedia, et enregistre le résultat dans le fichier sparql.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Requêtes : <a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fplace+%3Fname+%3Flocation+%3Flocname+%3Fdate+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fplace+rdf%3Atype+dbo%3AHistoricPlace+%3B%0A+++++++++++++++++++foaf%3Aname+%3Fname+%3B%0A+++++++++++++++++++dbp%3Abuilt+%3Fdate+%3B%0A+++++++++++++++++++dbo%3Alocation+%3Flocation+%3B%0A+++++++++++++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A+++++++++++++++++++geo%3Alat+%3Flat+%3B%0A+++++++++++++++++++geo%3Along+%3Flon+%3B%0A+++++++++++++++++++dbo%3Athumbnail+%3Fphoto+%3B%0A+++++++++++++++++++dbo%3Aabstract+%3Fabstract+.%0A++++%3Flocation+rdfs%3Alabel+%3Flocname%0A++FILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A++FILTER+langMatches%28lang%28%3Flocname%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=text%2Fhtml&timeout=30000&signal_void=on&signal_unconnected=on\">HTML</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fplace+%3Fname+%3Flocation+%3Flocname+%3Fdate+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fplace+rdf%3Atype+dbo%3AHistoricPlace+%3B%0A+++++++++++++++++++foaf%3Aname+%3Fname+%3B%0A+++++++++++++++++++dbp%3Abuilt+%3Fdate+%3B%0A+++++++++++++++++++dbo%3Alocation+%3Flocation+%3B%0A+++++++++++++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A+++++++++++++++++++geo%3Alat+%3Flat+%3B%0A+++++++++++++++++++geo%3Along+%3Flon+%3B%0A+++++++++++++++++++dbo%3Athumbnail+%3Fphoto+%3B%0A+++++++++++++++++++dbo%3Aabstract+%3Fabstract+.%0A++++%3Flocation+rdfs%3Alabel+%3Flocname%0A++FILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A++FILTER+langMatches%28lang%28%3Flocname%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=application%2Fsparql-results%2Bjson&timeout=30000&signal_void=on&signal_unconnected=on\">JSON</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fplace+%3Fname+%3Flocation+%3Flocname+%3Fdate+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fplace+rdf%3Atype+dbo%3AHistoricPlace+%3B%0A+++++++++++++++++++foaf%3Aname+%3Fname+%3B%0A+++++++++++++++++++dbp%3Abuilt+%3Fdate+%3B%0A+++++++++++++++++++dbo%3Alocation+%3Flocation+%3B%0A+++++++++++++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A+++++++++++++++++++geo%3Alat+%3Flat+%3B%0A+++++++++++++++++++geo%3Along+%3Flon+%3B%0A+++++++++++++++++++dbo%3Athumbnail+%3Fphoto+%3B%0A+++++++++++++++++++dbo%3Aabstract+%3Fabstract+.%0A++++%3Flocation+rdfs%3Alabel+%3Flocname%0A++FILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A++FILTER+langMatches%28lang%28%3Flocname%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=text%2Fcsv&timeout=30000&signal_void=on&signal_unconnected=on\">CSV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_filename = 'sparql'\n",
    "dbpedia_sparql_to_csv(raw_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.4 Nettoyage des données__\n",
    "\n",
    "La cellule suivante relit le fichier des données brutes dans le dictionnaire nommé <code>sites</code>, puis les cellules consécutives modifient ces données en mémoire pour les nettoyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Lecture du fichier d'origine, avec suppression des doublons\n",
    "#\n",
    "import csv\n",
    "\n",
    "places = {}\n",
    "with open('{}.csv'.format(raw_filename),encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=',')\n",
    "    for row in reader:\n",
    "        if not row['name'] in places:\n",
    "            places[row['name']] = row\n",
    "\n",
    "print(len(places))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Elimination des doublons\n",
    "#\n",
    "special_wikinames = [\n",
    "    \"Kayak Island\",\n",
    "    \"Massachusetts Museum of Contemporary Art\",\n",
    "    \"National Flag Memorial (Argentina)\"\n",
    "]\n",
    "check_wiki = {}\n",
    "for p in [p for p in places]:\n",
    "    wikiname = places[p]['wiki'].split('/')[-1].replace('_',' ')\n",
    "\n",
    "    # renommage\n",
    "    if wikiname in special_wikinames :\n",
    "        places[p]['name'] = wikiname\n",
    "        places[wikiname] = places[p]\n",
    "        del places[p]\n",
    "        p = wikiname\n",
    "        \n",
    "    if places[p]['name'] == '':\n",
    "        del places[p]  \n",
    "    elif not places[p]['wiki'] in check_wiki:\n",
    "        check_wiki[places[p]['wiki']] = p\n",
    "    else :\n",
    "        if wikiname == places[check_wiki[places[p]['wiki']]]['name']:\n",
    "            #print('keeping',wikiname,'vs.',places[p]['name'])\n",
    "            del places[p]\n",
    "        elif wikiname == places[p]['name']:\n",
    "            #print('replacing',places[check_wiki[places[p]['wiki']]]['name'],'by',places[p]['name'])\n",
    "            del places[check_wiki[places[p]['wiki']]]\n",
    "        else:\n",
    "            print(check_wiki[places[p]['wiki']],'/',places[p]['name'], places[p]['wiki'])\n",
    "            print(p)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### # Traitement des dates\n",
    "#\n",
    "fix_dates = {\n",
    "    # > 2021\n",
    "    \"Pike Place Public Market Historic District\": \"1907\",\n",
    "    \"Sweet Track\": \"3807 avant J.C.\",\n",
    "    \"Victoria Memorial\": \"1901-1924\",\n",
    "    # < 1000\n",
    "    \"Chetro Ketl\": \"990-1075\",\n",
    "    \"Obelisk of Theodosius\": \"de 1479 à 1425 avant J.C.\",\n",
    "    \"Temple of Debod\": \"200 avant J.C.\",\n",
    "    \"The Miami Circle at Brickell Point Site\": \"500 avant J.C. ?\",\n",
    "    # Exception\n",
    "    \"Borobudur\": \"9ème siècle\",\n",
    "\t\"Chattanooga National Cemetery\": \"1863\",\n",
    "\t\"Chinese Pavilion at Drottningholm\": \"1753\",\n",
    "\t\"Church of San Dionisio\": \"fin du 15ème siècle\",\n",
    "\t\"Château d'Agnou\": \"fin du 16ème siècle\",\n",
    "\t\"Château de Lichtenberg\": \"aux environs de 1200\",\n",
    "\t\"Châteaux of the Loire Valley\": \"lors de la Renaissance\",\n",
    "\t\"Ducal Palace of Gandia\": \"à partir du 14ème siècle\",\n",
    "\t\"Fort Charlesbourg Royal\": \"été 1541\",\n",
    "\t\"Fort Lyon\": \"1867\",\n",
    "\t\"Fredericton City Hall\": \"1875\",\n",
    "\t\"Freedom Square – Liberty Square\": \"début 19ème\",\n",
    "\t\"Garegin Nzhdeh Square\": \"1959\",\n",
    "\t\"Green Gables Heritage Place\": \"1830\",\n",
    "\t\"Hjaltadans\": \"au Néolithique\",\n",
    "\t\"Jew's House, Lincoln\": \"fin du 12ème siècle\",\n",
    "\t\"King-Walker Place\": \"1870\",\n",
    "\t\"Law Uk Hakka House\": \"1750\",\n",
    "\t\"Lyme Park\": \"1720\",\n",
    "\t\"Maynooth Castle\": \"fin du 12ème siècle\",\n",
    "\t\"Medina of Sousse\": \"7ème siècle\",\n",
    "\t\"Mendut\": \"9ème siècle\",\n",
    "\t\"Mississippi State Capitol\": \"1901\",\n",
    "\t\"Monument to the Divine Savior of the World\": \"1942\",\n",
    "\t\"Monument to the Independence of Brazil\": \"1884\",\n",
    "\t\"Mseilha Fort\": \"13ème siècle\",\n",
    "\t\"Mélusine tower\": \"fin 12ème / début 13ème\",\n",
    "\t\"Norwich Castle\": \"1067\",\n",
    "\t\"Obelisco de Buenos Aires\": \"1936\",\n",
    "\t\"Old Louisville Residential District\": \"1850\",\n",
    "\t\"Palais Leuchtenberg\": \"Original 1817\",\n",
    "\t\"Pawon\": \"9ème siècle\",\n",
    "\t\"Petra\": \"vers le 5ème siècle avant J.C.\",\n",
    "\t\"Prambanan\": \"850\",\n",
    "\t\"Pullman National Monument\": \"1880\",\n",
    "\t\"Red Fort\": \"1639\",\n",
    "\t\"Roddenbury Hillfort\": \"durant l'âge du Fer\",\n",
    "\t\"Royal Tombs of the Joseon Dynasty\": \"entre 1392 et 1897\",\n",
    "\t\"Scord of Brouster\": \"au Neolithique\",\n",
    "\t\"Selamat Datang Monument\": \"1961\",\n",
    "\t\"Solsbury Hill\": \"durant l'âge du Fer\",\n",
    "\t\"Stourhead\": \"1721\",\n",
    "\t\"Taq Kasra\": \"3ème siècle\",\n",
    "\t\"The Monument of Independence\": \"1910\",\n",
    "\t\"The Westin Palace Madrid\": \"1912\",\n",
    "\t\"Torre Monumental\": \"1916\",\n",
    "\t\"Tower of London\": \"1078\",\n",
    "\t\"Tung Chung Fort\": \"1174\",\n",
    "\t\"Tyson McCarter Place\": \"vers 1876\",\n",
    "\t\"Vénus de Quinipily\": \"vers 49 avant J.C.\",\n",
    "\t\"White Elephant\": \"1938\",\n",
    "    \"Al-Jdayde (Aleppo)\": \"fin du 14ème siècle\",\n",
    "    \"Archiepiscopal Palace of Rouen\" : \"13ème siècle\",\n",
    "    \"Church of St Martin\": \"avant 597\",\n",
    "    \"Riverdale–Spuyten Duyvil–Kingsbridge Memorial Bell Tower\": \"1930\",\n",
    "    \"Vivekananda Rock Memorial\": \"1970\"\n",
    "}\n",
    "bc_dates = [\n",
    "    \"Ambrussum\",\n",
    "    \"Royal Mausoleum of Mauretania (Caesariensis)\",\n",
    "    \"Temple of Eshmun\",\n",
    "    \"Tomb of Cyrus the Great\",\n",
    "]\n",
    "for p in [p for p in places]:\n",
    "    date = places[p]['date']\n",
    "    if date.startswith('c. ') :\n",
    "        date = 'environ ' + date[3:]\n",
    "    elif date.startswith('c.') :\n",
    "        date = 'environ ' + date[2:]\n",
    "    elif date.startswith('*') :\n",
    "        date = date[1:5]\n",
    "    elif places[p]['name'] in fix_dates and fix_dates[places[p]['name']]:\n",
    "        date = fix_dates[places[p]['name']]\n",
    "    elif places[p]['name'] in fix_dates:\n",
    "        print(date, '|', places[p]['name'], places[p]['wiki'])\n",
    "    else :\n",
    "        try:\n",
    "            y = int(places[p]['date'])\n",
    "            if places[p]['name'] in bc_dates:\n",
    "                date = '{}ème siècle avant J.C.'.format(y)    \n",
    "            elif y < 250:\n",
    "                date = '{}ème siècle'.format(y)\n",
    "            else:\n",
    "                date = '{}'.format(y)\n",
    "        except:\n",
    "            print(date, '|', places[p]['name'], places[p]['wiki'])\n",
    "            pass\n",
    "    places[p]['date'] = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Mise à jour des lieux\n",
    "#\n",
    "fix_locations = {\n",
    "    \"Alcázar of the Christian Kings\": \"Córdoba (Espagne)\",\n",
    "    \"Arles Obelisk\": \"Arles (France)\",\n",
    "    \"Ballymoon Castle\": \"Muine Bheag (Irlande)\",\n",
    "    \"Borobudur\": \"Magelang (Java)\",\n",
    "    \"Camden Town Hall\": \"Londres\",\n",
    "    \"Chicago Avenue Water Tower andPumping Station\": \"Chicago\",\n",
    "    \"Château de la Guignardière\": \"Avrillé (France)\",\n",
    "    \"Congress Column\": \"Bruxelles\",\n",
    "    \"Coral Castle\": \"Floride\",\n",
    "    \"De Soto National Memorial\": \"Bradenton (Floride)\",\n",
    "    \"Fort Saint-André\": \"département du Gard (France)\",\n",
    "    \"Gedong Songo\": \"Bandungan (Java)\",\n",
    "    \"George Rogers Clark National Historical Park\": \" Vincennes (Indiana)\",\n",
    "    \"Gołuchów Castle\": \"Gołuchów (Pologne)\",\n",
    "    \"Mont des Arts\": \"Bruxelles\",\n",
    "    \"Obelisco de Buenos Aires\": \"Argentine\",\n",
    "    \"Prambanan\": \"Indonésie\",\n",
    "    \"President's Park\": \"Washington, D.C\",\n",
    "    \"Royal Victoria Patriotic Building\": \"Londres\",\n",
    "    \"Sweet Track\": \"Angleterre\",\n",
    "    \"Taliesin\": \" Spring Green, Wisconsin\",\n",
    "    \"The Charterhouse, London\": \"Islington (Londres)\",\n",
    "    \"Tower of London\": \"Tower Hamlets (Londres)\",\n",
    "    \"Victoria Memorial\": \"The Mall (Londres)\",\n",
    "    \"White Lodge\": \"Richmond upon Thames (Londres)\"\n",
    "}\n",
    "for p in fix_locations:\n",
    "    if p in places:\n",
    "        places[p]['locname'] = fix_locations[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Certains enregistrement font référence à des photos non disponibles (404 Not Found)\n",
    "#\n",
    "# Cette cellule met manuellement à jour ces enregistrements avec des photos accessibles\n",
    "#\n",
    "photos = {\n",
    "\t\"Studenica Monastery\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Studenica_monastery_%28Manastir_Studenica%29_-_by_Pudelek.jpg/320px-Studenica_monastery_%28Manastir_Studenica%29_-_by_Pudelek.jpg\",\n",
    "\n",
    "}\n",
    "for p in photos:\n",
    "    places[p]['photo'] = photos[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Fonctions pour la recherche des images aux liens erronés\n",
    "#\n",
    "import urllib.parse\n",
    "import http.client\n",
    "import time\n",
    "\n",
    "#\n",
    "# envoi d'une requête hhtp\n",
    "#\n",
    "def http_request(url):\n",
    "    # print('hello from http_request',url)\n",
    "    \n",
    "    (baseurl,querystring) = url.split('?',1) if '?' in url else (url,'')\n",
    "    (protocol,netpath) = baseurl.split(':',1)\n",
    "    (_,__,server,path) = netpath.split('/',3)\n",
    "    path = '/'.join([urllib.parse.quote(chunk) for chunk in path.split('/')])\n",
    "\n",
    "    conn = http.client.HTTPSConnection(server)\n",
    "    conn.request('HEAD','/'+path+'?'+querystring)\n",
    "    r = conn.getresponse()\n",
    "    \n",
    "    if ( r.status == 200 ):\n",
    "        return 200\n",
    "    elif ( r.status == 404 ):\n",
    "        return 404\n",
    "    elif ( r.status == 302 ):\n",
    "        # print ('302', 'redirecting to ',r.headers['Location'])\n",
    "        return http_request(r.headers['Location'])\n",
    "    elif ( r.status == 301 ):\n",
    "        # print ('301', 'redirecting to ',r.headers['Location'])\n",
    "        return http_request(r.headers['Location'])\n",
    "    else:\n",
    "        return r.status\n",
    "\n",
    "#\n",
    "# liste des photos aux liens erronés\n",
    "#\n",
    "failed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Mise à jour de la liste des photos aux liens erronnés\n",
    "#\n",
    "# Pour parcourir l'ensemble des ponts, modifier les variables start et end.\n",
    "#\n",
    "# ATTENTION : cette procédure est potentiellement très lente puisqu'elle effectue une requête\n",
    "# pour vérifier chacune des images, et il y a plusieurs centaines de ponts...\n",
    "#\n",
    "start = 0\n",
    "end = 10\n",
    "\n",
    "#\n",
    "# test photos\n",
    "#\n",
    "for p in [p for p in places][start:end]:\n",
    "    status = http_request(places[p]['photo'])\n",
    "    if ( status == 404 ):\n",
    "        print (status,places[p]['name'],'\\n',places[p]['wiki'],'\\n')\n",
    "        if not p in failed :\n",
    "            failed.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.5 Ecriture du fichier des données nettoyées__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['place', 'name', 'location', 'locname', 'date', 'wiki', 'lat', 'lon', 'abstract', 'photo']\n",
      "['place', 'name', 'location', 'date', 'wiki', 'lat', 'lon', 'abstract', 'photo']\n"
     ]
    }
   ],
   "source": [
    "#           \n",
    "# Ecriture du fichier csv à importer dans la base de données\n",
    "#\n",
    "fieldnames = list(places[\"Ambrussum\"].keys())\n",
    "print(fieldnames)\n",
    "\n",
    "for p in places:\n",
    "    places[p]['location'] = places[p]['locname']\n",
    "\n",
    "fieldnames.remove('locname')\n",
    "\n",
    "print(fieldnames)\n",
    "\n",
    "with open('lieux.csv', 'w', encoding='utf-8', newline='\\n') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    for p in places:\n",
    "        writer.writerow({f: places[p][f] for f in fieldnames})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.6 Création / mise à jour de la base de données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Mise à jour de la base de données\n",
    "#\n",
    "import sqlite3\n",
    "\n",
    "places_dbname = 'lieux.db'\n",
    "conn = sqlite3.connect(places_dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"DROP TABLE IF EXISTS lieux\")\n",
    "conn.commit()\n",
    "\n",
    "c.execute('''CREATE TABLE \"lieux\" (\n",
    "    `place` TEXT PRIMARY KEY,\n",
    "    `name` TEXT,\n",
    "    `location` TEXT,\n",
    "    `date` TEXT,\n",
    "    `wiki` TEXT,\n",
    "    `lat` REAL,\n",
    "    `lon` REAL,\n",
    "    `abstract` TEXT,\n",
    "    `photo` TEXT\n",
    ")''')\n",
    "conn.commit()\n",
    "\n",
    "request = 'INSERT INTO lieux ({}) VALUES ({})'.format(','.join(fieldnames),','.join(['?']*len(fieldnames)))\n",
    "for p in places:\n",
    "    c.execute(request,[places[p][k] for k in fieldnames])\n",
    "    \n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
