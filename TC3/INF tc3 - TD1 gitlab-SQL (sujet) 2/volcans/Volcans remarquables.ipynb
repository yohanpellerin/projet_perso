{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center style=\"color: #66d\">Projet A - Volcans remarquables</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Origine des données - DBPedia\n",
    "\n",
    "Cette année, le principe de chacun des projets consiste à récupérer des données sur DBPedia. \n",
    "\n",
    "<img src=\"DBPedia.png\" width=\"120\">\n",
    "\n",
    "DBpedia est un projet universitaire et communautaire d'exploration et extraction automatique de données dérivées de Wikipédia. Son principe est de proposer une version structurée et sous forme de données normalisées au format RDF des contenus encyclopédiques de chaque page de Wikipédia. Il existe plusieurs versions de DBpedia et dans plusieurs langues. Les trois versions principales sont la version anglaise (http://dbpedia.org/sparql), la versions française (http://fr.dbpedia.org) et la version allemande (http://de.dbpedia.org/).\n",
    "\n",
    "La version qui a été utilisée pour récupérer les données qui vous sont fournies est la version anglaise c’est à dire http://dbpedia.org/ car c’est la plus complète. Cependant il est tout à fait possible d’adapter les différentes requêtes aux autres chapitres multilingues de DBpedia (ex: la version française) en tenant compte des différences entre les chapitres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Format des données - RDF\n",
    "\n",
    "Sur DBPedia, les données sont représentées au format\n",
    "<a href=\"https://fr.wikipedia.org/wiki/Resource_Description_Framework\">RDF</a>\n",
    "(Resource Description Framework). RDF est un modèle de graphe destiné à décrire de façon formelle des ressources et leurs métadonnées, de façon à permettre le traitement automatique de telles descriptions. Développé par le <a href=\"https://www.w3.org/\">World Wide Web Consortium</a> (W3C en abrégé), RDF est un des langages de base du Web sémantique.\n",
    "\n",
    "<img src=\"Rdf_logo.svg\" width=\"100\">\n",
    "\n",
    "Un document RDF est constitué d'un ensemble de triplets. Un triplet RDF est une association (sujet, prédicat, objet).\n",
    "\n",
    "* Le sujet représente la ressource à décrire.\n",
    "* Le prédicat est une propriété de la ressource.\n",
    "* L’objet donne la valeur de la propriété, et peut correspondre à une donnée numérique ou textuelle, ou à autre ressource.\n",
    "\n",
    "Les ressources et les prédicats sont représentés à l'aide d'une URL.\n",
    "\n",
    "Exemple : pour observer l'ensemble des propriétés de la ressource <code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Acatenango&gt;</code> avec leur valeur il suffit d'actionner le lien : http://dbpedia.org/resource/Acatenango\n",
    "\n",
    "On y apprend ainsi que :<br>\n",
    "<code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Acatenango&gt;</code> <code>dbo:elevation 3976.000000</code>\n",
    "\n",
    "Ce qui peut s'exprimer en français par : L'Acatenango s'élève à une altitude (dbpedia ontology : elevation) de 3976 m.\n",
    "\n",
    "Note: pour représenter les URLs des ressources et des propriétés, DBPedia utilise un certain nombre de préfixes prédéfinis. Ainsi l'URL <code>&lt;dbo:elevation&gt;</code> correspond à <code>&lt;http://</code><code>dbpedia.org</code><code>/ontology/elevation&gt;</code> obtenue en remplaçant le préfixe par sa valeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Récupération de données sur DBPedia - SPARQL\n",
    "\n",
    "<a href=\"https://fr.wikipedia.org/wiki/SPARQL\">SPARQL</a> est un langage de requête et un protocole qui permet de rechercher, d'ajouter, de modifier ou de supprimer des données RDF disponibles à travers Internet. Son nom est un acronyme récursif qui signifie : \"SPARQL Protocol And RDF Query Language\".\n",
    "\n",
    "Voici un exemple simple de requête SPARQL, qui s'interprète comme \"Quelle est la ressource dont le sujet principal est l'Acatenango ?\", et va nous retourner l'adresse de la page wikipédia consacrée à l'Acatenango :\n",
    "\n",
    "<code>SELECT ?wiki  WHERE { </code><code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Acatenango&gt;</code><code>  foaf:isPrimaryTopicOf  ?wiki  }</code>\n",
    "\n",
    "Il est possible de soumettre de telles requêtes sur le point d'accès dédié de DBPedia : http://dbpedia.org/sparql.\n",
    "\n",
    "<div style=\"background-color:#eef;padding:10px;border-radius:3px; margin-top: 1.33em\">\n",
    "Soumettez la requête précédente via le point d'accès SPARQL de DBPedia pour observer\n",
    "la réponse obtenue, et vérifier que l'information retournée correspond bien à l'adresse de la page wikipédia demandée : <code>http://en.wikipedia.org/wiki/Acatenango</code>.\n",
    "</div>\n",
    "\n",
    "SPARQL est un langage puissant, qui permet d'émettre des requêtes complexes. Pour plus d'informations sur SPARQL et la façon d'utiliser DBPedia, il ne sera pas inutile de consulter le <a href=\"http://fr.dbpedia.org/sparqlTuto/tutoSparql.html\">tutoriel en ligne</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Données sur les volcans\n",
    "\n",
    "Les ressources concernant les volcans disponibles sur DBPedia sont du type <a href=\"https://dbpedia.org/ontology/Volcano\"><code>dbo:Volcano</code></a>. La requête suivante en demande la liste, avec leur nom, l'adresse de la page Wikipédia qui les décrit, leur altitude, latitude, longitude, la date ou l'époque de leur dernière éruption, le texte et l'URL de l'image qui les décrivent sur Wikipédia :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "sparql"
   },
   "source": [
    "SELECT DISTINCT ?volcano ?name ?wiki ?elevation ?lat ?lon ?year ?eruption ?abstract ?photo\n",
    "WHERE {\n",
    "    ?volcano rdf:type dbo:Volcano ;\n",
    "                   foaf:name ?name ;\n",
    "                   foaf:isPrimaryTopicOf ?wiki ;\n",
    "                   dbp:elevationM ?elevation ;\n",
    "                   geo:lat ?lat ;\n",
    "                   geo:long ?lon ;\n",
    "                   dbp:lastEruption ?eruption ;\n",
    "                   dbo:thumbnail ?photo ;\n",
    "                   dbo:abstract ?abstract\n",
    "    OPTIONAL {\n",
    "        ?volcano dbo:eruptionYear ?year\n",
    "    }  \n",
    "FILTER langMatches(lang(?abstract), 'fr')\n",
    "}\n",
    "ORDER BY (?name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque importante__\n",
    "\n",
    "<p>Cela n'est pas nécessaire dans l'immédiat, mais si vous désirez réinitialiser le contenu de votre base de données, il faudra exécuter dans l'ordre, l'ensemble des cellules présentes dans la suite de ce notebook.  Si les données sources ont été modifiées, il faudra peut-être intervenir à la marge sur certaines parties du code de nettoyage des données.</p>\n",
    "\n",
    "<p>De même, pour compléter et/ou modifier vos données, vous devrez modifier la requête SPARQL présente dans la cellule ci-dessus, et éventuellement nettoyer les nouvelles données obtenues en complétant le notebook avec le code python nécessaire.\n",
    "</p>\n",
    "\n",
    "<p>Toutefois, avant de vous aventurer à modifier la requête SPARQL, il sera pertinent de tester votre nouvelle requête via le point d'entrée interactif de DBPedia, en ajoutant une clause LIMIT(10) par exemple, pour éviter de surcharger le serveur, et d'être obligé d'attendre les résultats trop longtemps.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.1 Enregistrement du notebook et récupération de son nom dans la variable notebook_name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
       "IPython.notebook.save_notebook()\n",
       "\n",
       "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"notebook_name = \" + \"'\"+thename+\"'\";\n",
       "kernel.execute(command);\n",
       "element.text(command)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
    "IPython.notebook.save_notebook()\n",
    "\n",
    "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"notebook_name = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);\n",
    "element.text(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.2 Définition des fonctions utilisées par la suite__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Emission d'un requête SPARQL vers le point d'entrée DBPedia\n",
    "# et récupération du résultat dans un fichier csv\n",
    "#\n",
    "# id : metadata.id de la cellule avec la requête SPARQL, et nom du fichier csv\n",
    "#\n",
    "def dbpedia_sparql_to_csv(cell_id):\n",
    "\n",
    "    query = get_cell_by_id(cell_id)['source']\n",
    "    url = display_dbpedia_links(query)['csv']\n",
    "    http_request_to_file(url,'{}.csv'.format(cell_id))\n",
    "\n",
    "#\n",
    "# Récupère une cellule du présent notebook\n",
    "#\n",
    "def get_cell_by_id(cell_id):\n",
    "\n",
    "    # https://discourse.jupyter.org/t/extract-specific-cells-from-students-notebooks/7951/4\n",
    "    import os\n",
    "    import nbformat as nbf\n",
    "    filename = \"{}.ipynb\".format(notebook_name)\n",
    "    notebook = nbf.read(filename, nbf.NO_CONVERT)\n",
    "    return [c for c in notebook.cells if 'id' in c['metadata'] and c['metadata']['id'] == cell_id][0]\n",
    "\n",
    "#\n",
    "# Renvoie l'url d'une requête vers le point d'entré SPARQL de DBPedia\n",
    "#\n",
    "def dbpedia_sparql_url(query,fmt):\n",
    "\n",
    "    # https://stackoverflow.com/questions/40557606/how-to-url-encode-in-python-3\n",
    "    from urllib.parse import urlencode, quote_plus\n",
    "    url = \"https://dbpedia.org/sparql\"\n",
    "    params = {\n",
    "        \"default-graph-uri\" : \"http://dbpedia.org\",\n",
    "        \"query\" : query,\n",
    "        \"format\" : fmt,\n",
    "        \"timeout\" : 30000,\n",
    "        \"signal_void\" : \"on\",\n",
    "        \"signal_unconnected\" : \"on\"\n",
    "    }\n",
    "    return \"{}?{}\".format(url,urlencode(params,quote_via=quote_plus))\n",
    "\n",
    "#\n",
    "# Affiche et renvoie les liens pour une requête SPARQL sur le point d'entrée DBPedia\n",
    "# avec un résultat au format html, json, ou csv\n",
    "#\n",
    "def display_dbpedia_links(query):\n",
    "    \n",
    "    html_url = dbpedia_sparql_url(query,'text/html')\n",
    "    json_url = dbpedia_sparql_url(query,'application/sparql-results+json')\n",
    "    csv_url = dbpedia_sparql_url(query,'text/csv')\n",
    "    \n",
    "    # https://stackoverflow.com/questions/48248987/inject-execute-js-code-to-ipython-notebook-and-forbid-its-further-execution-on-p\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    html_link = '<a href=\"{}\">HTML</a>'.format(html_url)\n",
    "    json_link = '<a href=\"{}\">JSON</a>'.format(json_url)\n",
    "    csv_link = '<a href=\"{}\">CSV</a>'.format(csv_url)\n",
    "\n",
    "    display(HTML('Requêtes : {}&nbsp;&nbsp;{}&nbsp;&nbsp;{}'.format(html_link,json_link,csv_link)))\n",
    "\n",
    "    return { \"html\": html_url, \"json\": json_url, \"csv\": csv_url}\n",
    "\n",
    "#\n",
    "# Emet une requête http et enregistre le résultat dans un fichier\n",
    "#\n",
    "def http_request_to_file(url,filename):\n",
    "    \n",
    "    # https://stackoverflow.com/questions/645312/what-is-the-quickest-way-to-http-get-in-python\n",
    "    import urllib.request\n",
    "    contents = urllib.request.urlopen(url).read()\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "        f.write(contents)\n",
    "\n",
    "#\n",
    "# Vérifie si une chaîne peut être convertie en float\n",
    "#\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__4.3 Récupération des données brutes provenant de DBPedia__\n",
    "\n",
    "Cette cellule envoie la requête SPARQL au serveur DBPedia, et enregistre le résultat dans le fichier sparql.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Requêtes : <a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fvolcano+%3Fname+%3Fwiki+%3Felevation+%3Flat+%3Flon+%3Fyear+%3Feruption+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fvolcano+rdf%3Atype+dbo%3AVolcano+%3B%0A+++++++++++++++++++foaf%3Aname+%3Fname+%3B%0A+++++++++++++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A+++++++++++++++++++dbp%3AelevationM+%3Felevation+%3B%0A+++++++++++++++++++geo%3Alat+%3Flat+%3B%0A+++++++++++++++++++geo%3Along+%3Flon+%3B%0A+++++++++++++++++++dbp%3AlastEruption+%3Feruption+%3B%0A+++++++++++++++++++dbo%3Athumbnail+%3Fphoto+%3B%0A+++++++++++++++++++dbo%3Aabstract+%3Fabstract%0A++++OPTIONAL+%7B%0A++++++++%3Fvolcano+dbo%3AeruptionYear+%3Fyear%0A++++%7D++%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=text%2Fhtml&timeout=30000&signal_void=on&signal_unconnected=on\">HTML</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fvolcano+%3Fname+%3Fwiki+%3Felevation+%3Flat+%3Flon+%3Fyear+%3Feruption+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fvolcano+rdf%3Atype+dbo%3AVolcano+%3B%0A+++++++++++++++++++foaf%3Aname+%3Fname+%3B%0A+++++++++++++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A+++++++++++++++++++dbp%3AelevationM+%3Felevation+%3B%0A+++++++++++++++++++geo%3Alat+%3Flat+%3B%0A+++++++++++++++++++geo%3Along+%3Flon+%3B%0A+++++++++++++++++++dbp%3AlastEruption+%3Feruption+%3B%0A+++++++++++++++++++dbo%3Athumbnail+%3Fphoto+%3B%0A+++++++++++++++++++dbo%3Aabstract+%3Fabstract%0A++++OPTIONAL+%7B%0A++++++++%3Fvolcano+dbo%3AeruptionYear+%3Fyear%0A++++%7D++%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=application%2Fsparql-results%2Bjson&timeout=30000&signal_void=on&signal_unconnected=on\">JSON</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fvolcano+%3Fname+%3Fwiki+%3Felevation+%3Flat+%3Flon+%3Fyear+%3Feruption+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fvolcano+rdf%3Atype+dbo%3AVolcano+%3B%0A+++++++++++++++++++foaf%3Aname+%3Fname+%3B%0A+++++++++++++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A+++++++++++++++++++dbp%3AelevationM+%3Felevation+%3B%0A+++++++++++++++++++geo%3Alat+%3Flat+%3B%0A+++++++++++++++++++geo%3Along+%3Flon+%3B%0A+++++++++++++++++++dbp%3AlastEruption+%3Feruption+%3B%0A+++++++++++++++++++dbo%3Athumbnail+%3Fphoto+%3B%0A+++++++++++++++++++dbo%3Aabstract+%3Fabstract%0A++++OPTIONAL+%7B%0A++++++++%3Fvolcano+dbo%3AeruptionYear+%3Fyear%0A++++%7D++%0AFILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A%7D%0AORDER+BY+%28%3Fname%29&format=text%2Fcsv&timeout=30000&signal_void=on&signal_unconnected=on\">CSV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_filename = 'sparql'\n",
    "dbpedia_sparql_to_csv(raw_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.4 Nettoyage des données__\n",
    "\n",
    "La cellule suivante relit le fichier des données brutes dans le dictionnaire nommé <code>volcans</code>, puis les cellules consécutives modifient ces données en mémoire pour les nettoyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Lecture du fichier d'origine, avec suppression des vrais-faux doublons\n",
    "#\n",
    "import csv\n",
    "\n",
    "volcans = {}\n",
    "with open('{}.csv'.format(raw_filename),encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=',')\n",
    "    for row in reader:\n",
    "        if not row['name'] in volcans:\n",
    "            volcans[row['name']] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Fix absence de nom\n",
    "#\n",
    "if '' in volcans:\n",
    "    del volcans['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted Volcano in Chile\n",
      "deleted Parinacota\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Chasse aux doublons non homonymes\n",
    "#\n",
    "to_be_deleted = []\n",
    "for v in volcans:\n",
    "    wiki = volcans[v]['volcano']\n",
    "    found = [k for k in volcans if volcans[k]['volcano'] == wiki]\n",
    "    if len(found) > 1 and not found[1] in to_be_deleted:\n",
    "        to_be_deleted.append(found[1])\n",
    "\n",
    "for k in to_be_deleted:\n",
    "    del volcans[k]\n",
    "    print('deleted {}'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Suppression des listes, en général non pertinentes\n",
    "#\n",
    "import re\n",
    "\n",
    "for v in volcans:\n",
    "    text = volcans[v]['abstract']\n",
    "    if '*' in text:\n",
    "        text = re.sub(r'(?smu)\\*.*$', '', text)\n",
    "        text = re.sub(r'\\s*:\\s*$', '.', text)\n",
    "        volcans[v]['abstract'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.5 Mise en forme des dates d'éruption__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Mise en forme et traduction des dates d'éruption\n",
    "#\n",
    "from datetime import date\n",
    "now = date.today().year\n",
    "\n",
    "periods = {\n",
    "    'Chibanian': ('au Chibanien (Pléistocène moyen)', -781000),\n",
    "    'Holocene': (\"durant l'Holocène\", -12000),\n",
    "    'Paleocene': ('au Paléocène', -66000000),\n",
    "    'Pleistocene': ('au Pléistocène', -2580000),\n",
    "    'Middle_Pleistocene': ('au Pléistocène', -2580000),\n",
    "}\n",
    "months = {\n",
    "    'january': 'janvier',\n",
    "    'february': 'février',\n",
    "    'march': 'mars',\n",
    "    'april': 'avril',\n",
    "    'may': 'mai',\n",
    "    'june': 'juin',\n",
    "    'july': 'juillet',\n",
    "    'august': 'août',\n",
    "    'september': 'septembre',\n",
    "    'october': 'octobre',\n",
    "    'november': 'novembre',\n",
    "    'december': 'décembre'\n",
    "}\n",
    "special = {\n",
    "    'Aguilera': ('il y a environ 3000 ans', -3000 + now),\n",
    "    'Mount Fuppushi': ('il y a 10000 ans', -10000 + now),\n",
    "    'Mount Kenya': (\"il y a 3.1 à 2.6 millions d'années\", -3100000 + now),\n",
    "    'Mount Sinabung': ('2021 (en cours)', now),\n",
    "    'Mount Amagi': (\"il y a 0.2 millions d'années\", -200000 + now),\n",
    "    'Aucanquilcha': ('Pléistocène', -2580000),\n",
    "    'Cerro Macá': ('1560', 1560),\n",
    "    'Coropuna': ('il y a environ 950 ans', -950 + now),\n",
    "    'Mount Kamui': ('1080', 1080),\n",
    "    'Pomerape': ('il y a environ 106000 ans', -106000 + now),\n",
    "    'The Cheviot': (\"il y a 393 millions d'années\", -393000000 + now),\n",
    "}\n",
    "\n",
    "for v in volcans:\n",
    "    year = volcans[v]['year'] if 'year' in volcans[v] else None \n",
    "    date = volcans[v]['eruption']\n",
    "    name = volcans[v]['name']\n",
    "    \n",
    "    # Unknown\n",
    "    if date.lower() == 'unknown' or date.lower() == \"'unknown'\":\n",
    "        date = \"date inconnue\"\n",
    "        year = -1000000000\n",
    "    \n",
    "    # Ongoing\n",
    "    elif date.lower() == 'ongoing':\n",
    "        date = \"en cours\"\n",
    "        year = now\n",
    "    \n",
    "    # Not in historic time\n",
    "    elif 'historic' in date:\n",
    "        date = \"aucune pendant les temps historiques\"\n",
    "        year = -1000000000\n",
    "        \n",
    "    # http://dbpedia.org/resource/Pleistocene\n",
    "    elif date.startswith('http:') or date in periods:\n",
    "        date, year = periods[date.split('/').pop()]\n",
    "    \n",
    "    # Holocene?\n",
    "    elif '?' in date:\n",
    "        period = periods[[k for k in periods.keys() if k in date][0]]\n",
    "        date = \"probablement {}\".format(period[0])\n",
    "        year = period[1]\n",
    "    \n",
    "    # possibly ...\n",
    "    elif 'possibly' in date.lower():\n",
    "        found = [k for k in periods.keys() if k in date]\n",
    "        \n",
    "        # Possibly Holocene\n",
    "        if len(found):\n",
    "            period = periods[found[0]]\n",
    "            date = \"peut-être {}\".format(period[0])\n",
    "            year = period[1]\n",
    "        \n",
    "        # Possibly 1251\n",
    "        else:\n",
    "            year = date.split().pop()\n",
    "            date = \"peut-être en {}\".format(year)\n",
    "            year = int(year)\n",
    "    \n",
    "    # Late Pleistocene\n",
    "    elif 'late' in date.lower():\n",
    "        found = [k for k in periods.keys() if k in date]\n",
    "        period = periods[found[0]]\n",
    "        date = \"{} tardif\".format(period[0])\n",
    "        year = period[1]\n",
    "    \n",
    "    # Pleistocene time\n",
    "    elif len([k for k in periods.keys() if k in date]):\n",
    "        date, year = periods[[k for k in periods.keys() if k in date][0]]\n",
    "    \n",
    "    # November to December 1972\n",
    "    elif len([k for k in months.keys() if k in date.lower()]):\n",
    "        found = [k for k in months.keys() if k in date.lower()]\n",
    "        for m in found:\n",
    "            date = date.lower().replace(m,months[m])\n",
    "        date = date.replace(' to ',' à ')\n",
    "        year = int(re.search(r'([0-9]+)',date).group(1))\n",
    "\n",
    "    # ca. 0.3-0.25 million years ago\n",
    "    elif 'ca.' in date:\n",
    "        date = \"il y a environ {} millions d'années\".format(date.split()[1].replace('-',' à '))\n",
    "        year = -300000 + now\n",
    "    \n",
    "    elif name in special:\n",
    "        date, year = special[name]\n",
    "       \n",
    "    # -350-01-01\n",
    "    elif year and year.split('-')[0] == '' :\n",
    "        year = year.split('-')[1]\n",
    "        date = 'vers {} AEC'.format(year)\n",
    "        year = -int(year)\n",
    "        \n",
    "    # 2460-01-01\n",
    "    elif year and float(year.split('-')[0]) > 2021:\n",
    "        year = year.split('-')[0]\n",
    "        date = 'il y a environ {} ans'.format(year)\n",
    "        year = -int(year) + now\n",
    "        \n",
    "    # 1707-01-01\n",
    "    elif year and float(year.split('-')[0]) > 1000:\n",
    "        date = year.split('-')[0]\n",
    "        year = int(date)\n",
    "\n",
    "    # 5000\n",
    "    elif isfloat(date) and float(date) > 2022 and float(date) < 10000 :\n",
    "        year = date\n",
    "        date = 'vers {} AEC'.format(date)\n",
    "        year = -int(year)\n",
    "        \n",
    "    # 0560-01-01\n",
    "    elif isfloat(date) and round(abs(float(date)) / 365.25 / 24 / 3600) < 1500:\n",
    "        year = int(year.split('-')[0])\n",
    "        date = \"vers l'an {} environ\".format(year)\n",
    "        year = int(year)\n",
    "    \n",
    "    # 6.31152E12\n",
    "    elif isfloat(date):\n",
    "        idate = round(abs(float(date)) / 365.25 / 24 / 3600)\n",
    "        date = \"il y a environ {} ans\".format(idate)\n",
    "        year = -idate + now\n",
    "            \n",
    "    # On ne devrait pas passer par ici\n",
    "    else:\n",
    "        print((year if year else '        ')+'\\t'+date+'\\t'+name+'\\t'+volcans[v]['wiki'])\n",
    "        pass\n",
    "    \n",
    "    # Mise à jour des dates dans le dictionnaire\n",
    "    volcans[v]['eruption_date'] = date\n",
    "    volcans[v]['eruption_year'] = year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.6 Ecriture du fichier des données nettoyées__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#           \n",
    "# Ecriture du fichier csv à importer dans la base de données\n",
    "#\n",
    "fieldnames = list(volcans['Mount Etna'].keys())\n",
    "fieldnames.remove('year')\n",
    "fieldnames.remove('eruption')\n",
    "\n",
    "with open('volcans.csv', 'w', encoding='utf-8', newline='\\n') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    for v in volcans:\n",
    "        writer.writerow({f: volcans[v][f] for f in fieldnames})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.7 Création / mise à jour de la base de données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relecture du fichier de données\n",
    "volcans = {}\n",
    "\n",
    "with open('volcans.csv',encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=';')\n",
    "    for row in reader:\n",
    "        volcans[row['name']] = row\n",
    "\n",
    "fieldnames = list(volcans['Mount Etna'].keys())\n",
    "\n",
    "#\n",
    "# Mise à jour de la base de données\n",
    "#\n",
    "import sqlite3\n",
    "\n",
    "volcano_dbname = 'volcans.db'\n",
    "conn = sqlite3.connect(volcano_dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"DROP TABLE IF EXISTS volcans\")\n",
    "conn.commit()\n",
    "\n",
    "c.execute('''CREATE TABLE \"volcans\" (\n",
    "    `volcano` TEXT PRIMARY KEY,\n",
    "    `name` TEXT,\n",
    "    `wiki` TEXT,\n",
    "    `elevation` INTEGER,\n",
    "    `lat` REAL,\n",
    "    `lon` REAL,\n",
    "    `eruption_date` TEXT,\n",
    "    `eruption_year` INTEGER,\n",
    "    `abstract` TEXT,\n",
    "    `photo` TEXT\n",
    ")''')\n",
    "conn.commit()\n",
    "\n",
    "request = 'INSERT INTO volcans ({}) VALUES ({})'.format(','.join(fieldnames),','.join(['?']*len(fieldnames)))\n",
    "for v in volcans:\n",
    "    c.execute(request,[volcans[v][k] for k in fieldnames])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
