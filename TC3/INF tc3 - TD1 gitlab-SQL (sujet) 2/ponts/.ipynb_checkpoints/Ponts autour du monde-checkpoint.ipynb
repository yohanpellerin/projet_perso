{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center style=\"color: #66d\">Projet D - Ponts autour du monde</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Origine des données - DBPedia\n",
    "\n",
    "Cette année, le principe de chacun des projets consiste à récupérer des données sur DBPedia. \n",
    "\n",
    "<img src=\"DBPedia.png\" width=\"120\">\n",
    "\n",
    "DBpedia est un projet universitaire et communautaire d'exploration et extraction automatique de données dérivées de Wikipédia. Son principe est de proposer une version structurée et sous forme de données normalisées au format RDF des contenus encyclopédiques de chaque page de Wikipédia. Il existe plusieurs versions de DBpedia et dans plusieurs langues. Les trois versions principales sont la version anglaise (http://dbpedia.org/sparql), la versions française (http://fr.dbpedia.org) et la version allemande (http://de.dbpedia.org/).\n",
    "\n",
    "La version qui a été utilisée pour récupérer les données qui vous sont fournies est la version anglaise c’est à dire http://dbpedia.org/ car c’est la plus complète. Cependant il est tout à fait possible d’adapter les différentes requêtes aux autres chapitres multilingues de DBpedia (ex: la version française) en tenant compte des différences entre les chapitres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Format des données - RDF\n",
    "\n",
    "Sur DBPedia, les données sont représentées au format\n",
    "<a href=\"https://fr.wikipedia.org/wiki/Resource_Description_Framework\">RDF</a>\n",
    "(Resource Description Framework). RDF est un modèle de graphe destiné à décrire de façon formelle des ressources et leurs métadonnées, de façon à permettre le traitement automatique de telles descriptions. Développé par le <a href=\"https://www.w3.org/\">World Wide Web Consortium</a> (W3C en abrégé), RDF est un des langages de base du Web sémantique.\n",
    "\n",
    "<img src=\"Rdf_logo.svg\" width=\"100\">\n",
    "\n",
    "Un document RDF est constitué d'un ensemble de triplets. Un triplet RDF est une association (sujet, prédicat, objet).\n",
    "\n",
    "* Le sujet représente la ressource à décrire.\n",
    "* Le prédicat est une propriété de la ressource.\n",
    "* L’objet donne la valeur de la propriété, et peut correspondre à une donnée numérique ou textuelle, ou à autre ressource.\n",
    "\n",
    "Les ressources et les prédicats sont représentés à l'aide d'une URL.\n",
    "\n",
    "Exemple : pour observer l'ensemble des propriétés de la ressource <code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Akashi_Kaikyō_Bridge&gt;</code> avec leur valeur il suffit d'actionner le lien : http://dbpedia.org/resource/Akashi_Kaikyō_Bridge\n",
    "\n",
    "On y apprend ainsi que :<br>\n",
    "<code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Akashi_Kaikyō_Bridge&gt;</code> <code>dbo:mainspan 1991</code>\n",
    "\n",
    "Ce qui peut s'exprimer en français par : Le pont du détroit d'Akashi possède une portée centrale (dbpedia ontolgy mainspan) de 1991 m.\n",
    "\n",
    "Note: pour représenter les URLs des ressources et des propriétés, DBPedia utilise un certain nombre de préfixes prédéfinis. Ainsi l'URL <code>&lt;dbo:mainspan&gt;</code> correspond à <code>&lt;http://</code><code>dbpedia.org</code><code>/ontology/mainspan&gt;</code> obtenue en remplaçant le préfixe par sa valeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Récupération de données sur DBPedia - SPARQL\n",
    "\n",
    "<a href=\"https://fr.wikipedia.org/wiki/SPARQL\">SPARQL</a> est un langage de requête et un protocole qui permet de rechercher, d'ajouter, de modifier ou de supprimer des données RDF disponibles à travers Internet. Son nom est un acronyme récursif qui signifie : \"SPARQL Protocol And RDF Query Language\".\n",
    "\n",
    "Voici un exemple simple de requête SPARQL, qui s'interprète comme \"Quelle est la ressource dont le sujet principal est le pont du détroit d'Akashi ?\", et va nous retourner l'adresse de la page wikipédia consacrée à ce pont :\n",
    "\n",
    "<code>SELECT ?wiki  WHERE { </code><code>&lt;http://</code><code>dbpedia.org</code><code>/resource/Akashi_Kaikyō_Bridge&gt;</code><code>  foaf:isPrimaryTopicOf  ?wiki  }</code>\n",
    "\n",
    "Il est possible de soumettre de telles requêtes sur le point d'accès dédié de DBPedia : http://dbpedia.org/sparql.\n",
    "\n",
    "<div style=\"background-color:#eef;padding:10px;border-radius:3px; margin-top: 1.33em\">\n",
    "Soumettez la requête précédente via le point d'accès SPARQL de DBPedia pour observer\n",
    "la réponse obtenue, et vérifier que l'information retournée correspond bien à l'adresse de la page wikipédia demandée : <code>http://en.wikipedia.org/wiki/Akashi_Kaikyō_Bridge</code>.\n",
    "</div>\n",
    "\n",
    "SPARQL est un langage puissant, qui permet d'émettre des requêtes complexes. Pour plus d'informations sur SPARQL et la façon d'utiliser DBPedia, il ne sera pas inutile de consulter le <a href=\"http://fr.dbpedia.org/sparqlTuto/tutoSparql.html\">tutoriel en ligne</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Données sur les ponts\n",
    "\n",
    "Les ressources concernant les ponts disponibles sur DBPedia sont du type <a href=\"http://dbpedia.org/ontology/Bridge\"><code>dbo:Bridge</code></a>. La requête suivante demande la liste des ponts, avec leur nom, leur longueur, date d'achèvement, l'adresse de la page Wikipédia qui les décrit, leur latitude, longitude, et le texte et l'URL de l'image qui les décrivent sur Wikipédia.\n",
    "\n",
    "Les données ainsi obtenues sont filtrées pour éliminer les rivières, canaux et digues qui se glissent sinon dans la liste obtenue :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "sparql"
   },
   "source": [
    "SELECT DISTINCT ?bridge ?name ?length ?span ?date ?wiki ?lat ?lon ?abstract ?photo\n",
    "WHERE {\n",
    "    ?bridge rdf:type dbo:Bridge ;\n",
    "        rdfs:label ?name ;\n",
    "        <http://dbpedia.org/ontology/Infrastructure/length> ?length ;\n",
    "        dbo:openingYear ?date ;\n",
    "        foaf:isPrimaryTopicOf ?wiki ;\n",
    "        geo:lat ?lat ;\n",
    "        geo:long ?lon ;\n",
    "        dbo:abstract ?abstract ;\n",
    "        dbo:thumbnail ?photo\n",
    "  OPTIONAL {\n",
    "    ?bridge dbo:mainspan ?span\n",
    "  }\n",
    "  FILTER langMatches(lang(?abstract), 'fr')\n",
    "  FILTER langMatches(lang(?name), 'fr')\n",
    "  FILTER( !EXISTS { ?bridge rdf:type dbo:River })\n",
    "  FILTER( !EXISTS { ?bridge rdf:type dbo:Canal })\n",
    "  FILTER( !EXISTS { ?bridge rdf:type dbo:Dam })\n",
    "}\n",
    "ORDER BY DESC(?span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque importante__\n",
    "\n",
    "<p>Cela n'est pas nécessaire dans l'immédiat, mais si vous désirez réinitialiser le contenu de votre base de données, il faudra exécuter dans l'ordre, l'ensemble des cellules présentes dans la suite de ce notebook. Si les données sources ont été modifiées, il faudra peut-être intervenir à la marge sur certaines parties du code de nettoyage des données.</p>\n",
    "\n",
    "<p>De même, pour compléter et/ou modifier vos données, vous devrez modifier la requête SPARQL présente dans la cellule ci-dessus, et éventuellement nettoyer les nouvelles données obtenues en complétant le notebook avec le code python nécessaire.\n",
    "</p>\n",
    "\n",
    "<p>Toutefois, avant de vous aventurer à modifier la requête SPARQL, il sera pertinent de tester votre nouvelle requête via le point d'entrée interactif de DBPedia, en ajoutant une clause LIMIT(10) par exemple, pour éviter de surcharger le serveur, et d'être obligé d'attendre les résultats trop longtemps.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.1 Enregistrement du notebook et récupération de son nom dans la variable notebook_name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
       "IPython.notebook.save_notebook()\n",
       "\n",
       "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"notebook_name = \" + \"\\\"\"+thename+\"\\\"\";\n",
       "kernel.execute(command);\n",
       "element.text(command)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "// Enregistrement des éventuelles modifications de la requête SPARQL\n",
    "IPython.notebook.save_notebook()\n",
    "\n",
    "// Enregistrement du nom du présent notebook dans la variable python notebook_name\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"notebook_name = \" + \"\\\"\"+thename+\"\\\"\";\n",
    "kernel.execute(command);\n",
    "element.text(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.2 Définition des fonctions utilisées par la suite__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Emission d'un requête SPARQL vers le point d'entrée DBPedia\n",
    "# et récupération du résultat dans un fichier csv\n",
    "#\n",
    "# id : metadata.id de la cellule avec la requête SPARQL, et nom du fichier csv\n",
    "#\n",
    "def dbpedia_sparql_to_csv(cell_id):\n",
    "\n",
    "    query = get_cell_by_id(cell_id)['source']\n",
    "    url = display_dbpedia_links(query)['csv']\n",
    "    http_request_to_file(url,'{}.csv'.format(cell_id))\n",
    "\n",
    "#\n",
    "# Récupère une cellule du présent notebook\n",
    "#\n",
    "def get_cell_by_id(cell_id):\n",
    "\n",
    "    # https://discourse.jupyter.org/t/extract-specific-cells-from-students-notebooks/7951/4\n",
    "    import os\n",
    "    import nbformat as nbf\n",
    "    filename = \"{}.ipynb\".format(notebook_name)\n",
    "    notebook = nbf.read(filename, nbf.NO_CONVERT)\n",
    "    return [c for c in notebook.cells if 'id' in c['metadata'] and c['metadata']['id'] == cell_id][0]\n",
    "\n",
    "#\n",
    "# Renvoie l'url d'une requête vers le point d'entré SPARQL de DBPedia\n",
    "#\n",
    "def dbpedia_sparql_url(query,fmt):\n",
    "\n",
    "    # https://stackoverflow.com/questions/40557606/how-to-url-encode-in-python-3\n",
    "    from urllib.parse import urlencode, quote_plus\n",
    "    url = \"https://dbpedia.org/sparql\"\n",
    "    params = {\n",
    "        \"default-graph-uri\" : \"http://dbpedia.org\",\n",
    "        \"query\" : query,\n",
    "        \"format\" : fmt,\n",
    "        \"timeout\" : 30000,\n",
    "        \"signal_void\" : \"on\",\n",
    "        \"signal_unconnected\" : \"on\"\n",
    "    }\n",
    "    return \"{}?{}\".format(url,urlencode(params,quote_via=quote_plus))\n",
    "\n",
    "#\n",
    "# Affiche et renvoie les liens pour une requête SPARQL sur le point d'entrée DBPedia\n",
    "# avec un résultat au format html, json, ou csv\n",
    "#\n",
    "def display_dbpedia_links(query):\n",
    "    \n",
    "    html_url = dbpedia_sparql_url(query,'text/html')\n",
    "    json_url = dbpedia_sparql_url(query,'application/sparql-results+json')\n",
    "    csv_url = dbpedia_sparql_url(query,'text/csv')\n",
    "    \n",
    "    # https://stackoverflow.com/questions/48248987/inject-execute-js-code-to-ipython-notebook-and-forbid-its-further-execution-on-p\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    html_link = '<a href=\"{}\">HTML</a>'.format(html_url)\n",
    "    json_link = '<a href=\"{}\">JSON</a>'.format(json_url)\n",
    "    csv_link = '<a href=\"{}\">CSV</a>'.format(csv_url)\n",
    "\n",
    "    display(HTML('Requêtes : {}&nbsp;&nbsp;{}&nbsp;&nbsp;{}'.format(html_link,json_link,csv_link)))\n",
    "\n",
    "    return { \"html\": html_url, \"json\": json_url, \"csv\": csv_url}\n",
    "\n",
    "#\n",
    "# Emet une requête http et enregistre le résultat dans un fichier\n",
    "#\n",
    "def http_request_to_file(url,filename):\n",
    "    \n",
    "    # https://stackoverflow.com/questions/645312/what-is-the-quickest-way-to-http-get-in-python\n",
    "    import urllib.request\n",
    "    contents = urllib.request.urlopen(url).read()\n",
    "\n",
    "    with open(filename,'wb') as f:\n",
    "        f.write(contents)\n",
    "\n",
    "#\n",
    "# Vérifie si une chaîne peut être convertie en float\n",
    "#\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.3 Récupération des données brutes provenant de DBPedia__\n",
    "\n",
    "Cette cellule envoie la requête SPARQL au serveur DBPedia, et enregistre le résultat dans le fichier sparql.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Requêtes : <a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fbridge+%3Fname+%3Flength+%3Fspan+%3Fdate+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fbridge+rdf%3Atype+dbo%3ABridge+%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++%3Chttp%3A%2F%2Fdbpedia.org%2Fontology%2FInfrastructure%2Flength%3E+%3Flength+%3B%0A++++++++dbo%3AopeningYear+%3Fdate+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0A++OPTIONAL+%7B%0A++++%3Fbridge+dbo%3Amainspan+%3Fspan%0A++%7D%0A++FILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A++FILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ARiver+%7D%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ACanal+%7D%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ADam+%7D%29%0A%7D%0AORDER+BY+DESC%28%3Fspan%29&format=text%2Fhtml&timeout=30000&signal_void=on&signal_unconnected=on\">HTML</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fbridge+%3Fname+%3Flength+%3Fspan+%3Fdate+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fbridge+rdf%3Atype+dbo%3ABridge+%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++%3Chttp%3A%2F%2Fdbpedia.org%2Fontology%2FInfrastructure%2Flength%3E+%3Flength+%3B%0A++++++++dbo%3AopeningYear+%3Fdate+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0A++OPTIONAL+%7B%0A++++%3Fbridge+dbo%3Amainspan+%3Fspan%0A++%7D%0A++FILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A++FILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ARiver+%7D%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ACanal+%7D%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ADam+%7D%29%0A%7D%0AORDER+BY+DESC%28%3Fspan%29&format=application%2Fsparql-results%2Bjson&timeout=30000&signal_void=on&signal_unconnected=on\">JSON</a>&nbsp;&nbsp;<a href=\"https://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=SELECT+DISTINCT+%3Fbridge+%3Fname+%3Flength+%3Fspan+%3Fdate+%3Fwiki+%3Flat+%3Flon+%3Fabstract+%3Fphoto%0AWHERE+%7B%0A++++%3Fbridge+rdf%3Atype+dbo%3ABridge+%3B%0A++++++++rdfs%3Alabel+%3Fname+%3B%0A++++++++%3Chttp%3A%2F%2Fdbpedia.org%2Fontology%2FInfrastructure%2Flength%3E+%3Flength+%3B%0A++++++++dbo%3AopeningYear+%3Fdate+%3B%0A++++++++foaf%3AisPrimaryTopicOf+%3Fwiki+%3B%0A++++++++geo%3Alat+%3Flat+%3B%0A++++++++geo%3Along+%3Flon+%3B%0A++++++++dbo%3Aabstract+%3Fabstract+%3B%0A++++++++dbo%3Athumbnail+%3Fphoto%0A++OPTIONAL+%7B%0A++++%3Fbridge+dbo%3Amainspan+%3Fspan%0A++%7D%0A++FILTER+langMatches%28lang%28%3Fabstract%29%2C+%27fr%27%29%0A++FILTER+langMatches%28lang%28%3Fname%29%2C+%27fr%27%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ARiver+%7D%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ACanal+%7D%29%0A++FILTER%28+%21EXISTS+%7B+%3Fbridge+rdf%3Atype+dbo%3ADam+%7D%29%0A%7D%0AORDER+BY+DESC%28%3Fspan%29&format=text%2Fcsv&timeout=30000&signal_void=on&signal_unconnected=on\">CSV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_filename = 'sparql'\n",
    "dbpedia_sparql_to_csv(raw_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.4 Nettoyage des données__\n",
    "\n",
    "La cellule suivante relit le fichier des données brutes dans le dictionnaire nommé <code>sites</code>, puis les cellules consécutives modifient ces données en mémoire pour les nettoyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Lecture du fichier d'origine, avec suppression des doublons\n",
    "#\n",
    "import csv\n",
    "bridges_raw_filename = 'sparql'\n",
    "\n",
    "bridges = {}\n",
    "with open('{}.csv'.format(bridges_raw_filename),encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=',')\n",
    "    for row in reader:\n",
    "        if not row['name'] in bridges:\n",
    "            bridges[row['name']] = row\n",
    "\n",
    "print(len(bridges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Traitement des dates\n",
    "#\n",
    "antique_bridges = [\n",
    "    'Pont Salario',\n",
    "    \"Pont sur l'Eurymédon\",\n",
    "    'Pont de Constantin le Grand'\n",
    "]\n",
    "fix_bridge_dates = {\n",
    "    'Pont de Shibanpo' : 1980,\n",
    "    'Pont des Tailleurs' : 1490,\n",
    "    \"Pont suspendu d'Ozimek\" : 1827,\n",
    "}\n",
    "\n",
    "for b in [b for b in bridges]:   \n",
    "    if bridges[b]['name'] in antique_bridges:\n",
    "        del bridges[b]\n",
    "        \n",
    "    elif bridges[b]['name'] in fix_bridge_dates :\n",
    "        bridges[b]['year'] = fix_bridge_dates[bridges[b]['name']] \n",
    "        \n",
    "    else:\n",
    "        bridges[b]['year'] = int(bridges[b]['date'].split('-')[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Traitement des longueurs\n",
    "#\n",
    "for b in bridges:\n",
    "    bridges[b]['length'] = float(bridges[b]['length'])\n",
    "    \n",
    "    #if not bridges[b]['span']:\n",
    "    #    print(bridges[b]['length'],bridges[b]['name'],bridges[b]['wiki'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Certains enregistrement font référence à des photos non disponibles (404 Not Found)\n",
    "#\n",
    "# Cette cellule met manuellement à jour ces enregistrements avec des photos accessibles\n",
    "#\n",
    "photos = {\n",
    "\t\"Pont de Normandie\": \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Pont_de_Normandie_%281%29.jpg\",\n",
    "    \"Pont de Karnali\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/I_captured_this_photo_in_Nepal_to_describes_Karnali_Bridge.jpg/320px-I_captured_this_photo_in_Nepal_to_describes_Karnali_Bridge.jpg\",\n",
    "    \"Pont Rio-Niterói\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Ilha_de_Mocangu%C3%AA_by_Diego_Baravelli_%28cropped%29.jpg/320px-Ilha_de_Mocangu%C3%AA_by_Diego_Baravelli_%28cropped%29.jpg\",\n",
    "    \"Pont de Bloukrans\" : \"https://files.structurae.net/files/photos/2094/1570_bloukrans_js_20625.jpg\",\n",
    "    \"Pont suspendu de Magapit\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Magapit_Bridge%2C_Lal-lo%2C_Cagayan.jpg/360px-Magapit_Bridge%2C_Lal-lo%2C_Cagayan.jpg\",\n",
    "    \"Pont de Chandani Dodhara\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Chadani-Dodhara_Bridge.JPG/320px-Chadani-Dodhara_Bridge.JPG\",\n",
    "    \"Pont Hong Kong-Zhuhai-Macao\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/West_section_of_Hong_Kong-Zhuhai-Macau_Bridge_%2820180902174105%29.jpg/320px-West_section_of_Hong_Kong-Zhuhai-Macau_Bridge_%2820180902174105%29.jpg\",\n",
    "    \"Pont de l'île de Ré\": \"https://upload.wikimedia.org/wikipedia/fr/c/c7/Pontre01.jpg\",\n",
    "}\n",
    "for b in photos:\n",
    "    bridges[b]['photo'] = photos[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Fonctions pour la recherche des images aux liens erronés\n",
    "#\n",
    "import urllib.parse\n",
    "import http.client\n",
    "import time\n",
    "\n",
    "#\n",
    "# envoi d'une requête hhtp\n",
    "#\n",
    "def http_request(url):\n",
    "    # print('hello from http_request',url)\n",
    "    \n",
    "    (baseurl,querystring) = url.split('?',1) if '?' in url else (url,'')\n",
    "    (protocol,netpath) = baseurl.split(':',1)\n",
    "    (_,__,server,path) = netpath.split('/',3)\n",
    "    path = '/'.join([urllib.parse.quote(chunk) for chunk in path.split('/')])\n",
    "\n",
    "    conn = http.client.HTTPSConnection(server)\n",
    "    conn.request('HEAD','/'+path+'?'+querystring)\n",
    "    r = conn.getresponse()\n",
    "    \n",
    "    if ( r.status == 200 ):\n",
    "        return 200\n",
    "    elif ( r.status == 404 ):\n",
    "        return 404\n",
    "    elif ( r.status == 302 ):\n",
    "        # print ('302', 'redirecting to ',r.headers['Location'])\n",
    "        return http_request(r.headers['Location'])\n",
    "    elif ( r.status == 301 ):\n",
    "        # print ('301', 'redirecting to ',r.headers['Location'])\n",
    "        return http_request(r.headers['Location'])\n",
    "    else:\n",
    "        return r.status\n",
    "\n",
    "#\n",
    "# liste des photos aux liens erronés\n",
    "#\n",
    "failed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Mise à jour de la liste des photos aux liens erronnés\n",
    "#\n",
    "# Pour parcourir l'ensemble des ponts, modifier les variables start et end.\n",
    "#\n",
    "# ATTENTION : cette procédure est potentiellement très lente puisqu'elle effectue une requête\n",
    "# pour vérifier chacune des images, et il y a plusieurs centaines de ponts...\n",
    "#\n",
    "start = 0\n",
    "end = 10\n",
    "\n",
    "#\n",
    "# test photos\n",
    "#\n",
    "for b in [b for b in bridges][start:end]:\n",
    "    status = http_request(bridges[b]['photo'])\n",
    "    if ( status == 404 ):\n",
    "        print (status,bridges[b]['name'],'\\n',bridges[b]['wiki'],'\\n')\n",
    "        if not b in failed :\n",
    "            failed.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/Broadway_Bridge_(Manhattan) Broadway Bridge\n",
      "http://dbpedia.org/resource/Broadway_Bridge_(Manhattan) Broadway\n"
     ]
    }
   ],
   "source": [
    "# suppression des doublons sur l'id\n",
    "bridges_by_id = {}\n",
    "for b in bridges:\n",
    "    id = bridges[b]['bridge']\n",
    "    if not id in bridges_by_id:\n",
    "        bridges_by_id[id] = bridges[b]\n",
    "    else:\n",
    "        print(bridges_by_id[id]['bridge'],bridges_by_id[id]['name'])\n",
    "        print(bridges[b]['bridge'],bridges[b]['name'])\n",
    "        bridges_by_id[id] = bridges[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.5 Ecriture du fichier des données nettoyées__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bridge', 'name', 'length', 'span', 'date', 'wiki', 'lat', 'lon', 'abstract', 'photo', 'year']\n",
      "['bridge', 'name', 'length', 'span', 'year', 'wiki', 'lat', 'lon', 'abstract', 'photo']\n"
     ]
    }
   ],
   "source": [
    "fieldnames = list(bridges[\"Pont de l'île de Ré\"].keys())\n",
    "print(fieldnames)\n",
    "\n",
    "fieldnames.remove('year')\n",
    "fieldnames = fieldnames[:4] + ['year'] + fieldnames[5:]\n",
    "print(fieldnames)\n",
    "\n",
    "\n",
    "#           \n",
    "# Ecriture du fichier csv à importer dans la base de données\n",
    "#\n",
    "with open('ponts.csv', 'w', encoding='utf-8', newline='\\n') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    for id in bridges_by_id:\n",
    "        writer.writerow({f: bridges_by_id[id][f] for f in fieldnames})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.6 Création / mise à jour de la base de données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# relecture du fichier de données\n",
    "bridges = {}\n",
    "\n",
    "with open('ponts.csv',encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=';')\n",
    "    for row in reader:\n",
    "        bridges[row['name']] = row\n",
    "\n",
    "fieldnames = list(bridges[\"Pont de l'île de Ré\"].keys())\n",
    "#\n",
    "# Mise à jour de la base de données\n",
    "#\n",
    "import sqlite3\n",
    "\n",
    "bridges_dbname = 'ponts.db'\n",
    "conn = sqlite3.connect(bridges_dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"DROP TABLE IF EXISTS ponts\")\n",
    "conn.commit()\n",
    "\n",
    "c.execute('''CREATE TABLE \"ponts\" (\n",
    "    `bridge` TEXT PRIMARY KEY,\n",
    "    `name` TEXT,\n",
    "    `length` REAL,\n",
    "    `span` REAL,\n",
    "    `year` INTEGER,\n",
    "    `wiki` TEXT,\n",
    "    `lat` REAL,\n",
    "    `lon` REAL,\n",
    "    `abstract` TEXT,\n",
    "    `photo` TEXT\n",
    ")''')\n",
    "conn.commit()\n",
    "\n",
    "request = 'INSERT INTO ponts ({}) VALUES ({})'.format(','.join(fieldnames),','.join(['?']*len(fieldnames)))\n",
    "for b in bridges:\n",
    "    c.execute(request,[bridges[b][k] for k in fieldnames])\n",
    "    \n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
